{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting numpy>=1.19.5 (from scikit-learn)\n",
      "  Downloading numpy-2.2.4-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp312-cp312-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 5.2/11.1 MB 31.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.1 MB 25.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 23.9 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading numpy-2.2.4-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 6.3/12.6 MB 29.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 30.5 MB/s eta 0:00:00\n",
      "Downloading scipy-1.15.2-cp312-cp312-win_amd64.whl (40.9 MB)\n",
      "   ---------------------------------------- 0.0/40.9 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 6.3/40.9 MB 38.6 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 7.3/40.9 MB 16.8 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 12.6/40.9 MB 22.5 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 17.6/40.9 MB 20.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 23.1/40.9 MB 23.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 24.4/40.9 MB 19.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 32.5/40.9 MB 21.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 39.3/40.9 MB 23.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 40.9/40.9 MB 22.5 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
      "Successfully installed joblib-1.4.2 numpy-2.2.4 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning\n",
    "- hyper parameter : 모델 설정과 관련해 직접 지정할 수 있는 매개변수\n",
    "- model parameter : 회귀계수(가중치), 절편 등 모델의 학습 대상이 되는 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV\n",
    "- 내 모델에서 가장 잘 맞는 하이퍼파라미터 조합을 자동으로 찾아주는 도구 (from scikit-learn 라이브러리)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 파라미터: {'n_neighbors': 7}\n",
      "최적의 모델 객체: KNeighborsClassifier(n_neighbors=7)\n",
      "최적화된 점수: 0.9800000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 데이터 로드\n",
    "iris_input, iris_target = load_iris(return_X_y=True)\n",
    "\n",
    "# 모델 생성\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# 테스트할 파라미터 값\n",
    "params = {\n",
    "    'n_neighbors': range(1, 13, 2)\n",
    "}\n",
    "\n",
    "# 첫 번째 인자: 모델\n",
    "# 두 번째 인자: 테스트 할 파라미터 (딕셔너리)\n",
    "# scoring: 평가 지표 (accuracy, precision, recall, f1)\n",
    "# cv: 반복 횟수, ex) 5-fold cross validation -> 데이터를 5덩어리로 나누어서 4개는 학습, 1개는 검증, 이거를 5번 반복.\n",
    "grid = GridSearchCV(knn, params, scoring='accuracy', cv=5) \n",
    "grid.fit(iris_input, iris_target)\n",
    "\n",
    "print(\"최적의 파라미터:\", grid.best_params_)\n",
    "print(\"최적의 모델 객체:\", grid.best_estimator_)\n",
    "print(\"최적화된 점수:\", grid.best_score_) # 각 반복에서 나온 점수의 평균을 grid.best_score_ 로 반환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_knn = grid.best_estimator_\n",
    "best_knn.fit(iris_input, iris_target)\n",
    "best_knn.score(iris_input, iris_target)\n",
    "# 최적의 모델 객체 n_neighbors': 7 로 학습 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomSearchCV\n",
    "- 하이퍼 파라미터의 값 목록이나 값의 범위를 제공하는데, 이 범위 중에 랜덤하게 값을 뽑아내 최적의 하이퍼 파라미터 조합을 찾는다.\n",
    "    - 탐색범위가 넓을 때 짧은 시간 내에 좋은 결과를 얻을 수 있다.\n",
    "    - 랜덤하게 값을 추출해 계산하므로, 전역 최적값을 놓칠 수 있다.\n",
    "\n",
    "##### 쉽게설명\n",
    "- 하이퍼파라미터 조합을 전부 다 해보는 대신, 일부만 랜덤하게 골라서 실험하는 방식\n",
    "\n",
    "🧩 다시 라면 예시! 🍜\n",
    "이번엔 라면 끓이는 실험을 다시 해봅시다.\n",
    "\n",
    "물 양: 400, 500, 600, 700, 800ml\n",
    "\n",
    "끓이는 시간: 2분, 3분, 4분, 5분\n",
    "\n",
    "계란: 있음, 없음\n",
    "\n",
    "고추가루: 있음, 없음\n",
    "\n",
    "모든 조합을 다 해보려면 5 × 4 × 2 × 2 = 80번 실험해야 해요.\n",
    "→ 이게 GridSearchCV 방식이에요.\n",
    "\n",
    "🍀 그런데 시간이 없다?\n",
    "\"그럼 10개만 랜덤하게 뽑아서 해보자!\"\n",
    "\n",
    "이게 바로 RandomizedSearchCV예요!\n",
    "빠르게 대강 실험해보고 적당히 괜찮은 조합을 찾는 거죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyuna\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] 지정된 파일을 찾을 수 없습니다\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\hyuna\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hyuna\\anaconda3\\envs\\pystudy_env\\Lib\\subprocess.py\", line 550, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hyuna\\anaconda3\\envs\\pystudy_env\\Lib\\subprocess.py\", line 1028, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\hyuna\\anaconda3\\envs\\pystudy_env\\Lib\\subprocess.py\", line 1540, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 파라미터: {'n_neighbors': 5}\n",
      "최적의 모델 객체: KNeighborsClassifier()\n",
      "최적화된 점수: 0.9733333333333334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.0007823 , 0.00062656, 0.00061197, 0.00089788, 0.00050635,\n",
       "        0.00080333, 0.00019574, 0.00039806, 0.00110941, 0.00040021]),\n",
       " 'std_fit_time': array([0.00048243, 0.00059352, 0.00049972, 0.00090413, 0.00101271,\n",
       "        0.0004017 , 0.00039148, 0.00048753, 0.00020987, 0.00049015]),\n",
       " 'mean_score_time': array([0.00295558, 0.00157471, 0.00147929, 0.023313  , 0.00171595,\n",
       "        0.00130739, 0.00820246, 0.00677481, 0.00100341, 0.00180302]),\n",
       " 'std_score_time': array([1.93830351e-03, 4.95114912e-04, 4.76295130e-04, 1.29360710e-02,\n",
       "        6.15590333e-04, 4.04482347e-04, 6.12701025e-04, 4.27007571e-04,\n",
       "        7.42029711e-06, 5.12467114e-04]),\n",
       " 'param_n_neighbors': masked_array(data=[57, 23, 21, 83, 5, 55, 77, 63, 45, 9],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value=999999),\n",
       " 'params': [{'n_neighbors': 57},\n",
       "  {'n_neighbors': 23},\n",
       "  {'n_neighbors': 21},\n",
       "  {'n_neighbors': 83},\n",
       "  {'n_neighbors': 5},\n",
       "  {'n_neighbors': 55},\n",
       "  {'n_neighbors': 77},\n",
       "  {'n_neighbors': 63},\n",
       "  {'n_neighbors': 45},\n",
       "  {'n_neighbors': 9}],\n",
       " 'split0_test_score': array([0.9       , 0.93333333, 0.93333333, 0.66666667, 0.96666667,\n",
       "        0.9       , 0.86666667, 0.9       , 0.9       , 0.96666667]),\n",
       " 'split1_test_score': array([0.93333333, 1.        , 1.        , 0.66666667, 1.        ,\n",
       "        0.93333333, 0.9       , 0.9       , 0.93333333, 1.        ]),\n",
       " 'split2_test_score': array([0.83333333, 0.93333333, 0.93333333, 0.66666667, 0.93333333,\n",
       "        0.83333333, 0.8       , 0.83333333, 0.9       , 0.96666667]),\n",
       " 'split3_test_score': array([0.93333333, 0.93333333, 0.96666667, 0.66666667, 0.96666667,\n",
       "        0.93333333, 0.93333333, 0.93333333, 0.96666667, 0.93333333]),\n",
       " 'split4_test_score': array([0.86666667, 1.        , 1.        , 0.63333333, 1.        ,\n",
       "        0.93333333, 0.86666667, 0.86666667, 1.        , 1.        ]),\n",
       " 'mean_test_score': array([0.89333333, 0.96      , 0.96666667, 0.66      , 0.97333333,\n",
       "        0.90666667, 0.87333333, 0.88666667, 0.94      , 0.97333333]),\n",
       " 'std_test_score': array([0.03887301, 0.03265986, 0.02981424, 0.01333333, 0.02494438,\n",
       "        0.03887301, 0.04422166, 0.03399346, 0.03887301, 0.02494438]),\n",
       " 'rank_test_score': array([ 7,  4,  3, 10,  1,  6,  9,  8,  5,  1], dtype=int32)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# 모델 생성\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# 테스트할 파라미터 생성\n",
    "params = {\n",
    "    'n_neighbors': range(1, 100, 2) # 1,3,5, ..., 99 \n",
    "}\n",
    "\n",
    "# n_iter: 탐색할 최적의 하이퍼 파라미터 조합 수 (기본값: 10) -> 후보 조합 중 10개를 랜덤하게 골라서 시도\n",
    "#         값이 크면 시간이 오래 걸림 / 값이 작으면 좋은 조합을 찾을 가능성 저하 \n",
    "# random_state=0: 랜덤 추출이지만 항상 동일한 결과를 원할 때 고정해주는 시드 값\n",
    "rd_search = RandomizedSearchCV(knn, params, cv=5, n_iter=10, random_state=0)\n",
    "rd_search.fit(iris_input, iris_target)\n",
    "\n",
    "print(\"최적의 파라미터:\", rd_search.best_params_)\n",
    "print(\"최적의 모델 객체:\", rd_search.best_estimator_)\n",
    "print(\"최적화된 점수:\", rd_search.best_score_)\n",
    "rd_search.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperOpt\n",
    "- 하이퍼파라미터를 \"똑똑하게\" 찾기 위한 자동 튜닝 라이브러리\n",
    "- (Python 기반 오픈소스, 딥러닝/머신러닝 모두에서 사용 가능!)\n",
    "\n",
    "📌 어디에 쓰일까?\n",
    "모델 학습할 때 중요한 하이퍼파라미터들:\n",
    "\n",
    "learning_rate\n",
    "\n",
    "batch_size\n",
    "\n",
    "n_estimators\n",
    "\n",
    "dropout_rate\n",
    "\n",
    "max_depth\n",
    "\n",
    "...\n",
    "\n",
    "이런 것들을 사람이 일일이 바꾸면서 실험하지 않고,\n",
    "HyperOpt가 스스로 탐색하면서 최적값을 찾아주는 역할을 해요.\n",
    "\n",
    "- 즉, 좋은 방향으로 똑똑하게 파라미터를 조정하고, 빠르고, 성능도 좋을 가능성이 높음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hyper.hp클래스**\n",
    "<table border=\"1\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>함수명</th>\n",
    "      <th>설명</th>\n",
    "      <th>사용 방법</th>\n",
    "      <th>예시 코드</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>hp.uniform</td>\n",
    "      <td>연속적인 실수 값 샘플링</td>\n",
    "      <td>hp.uniform(label, low, high)</td>\n",
    "      <td><code>hp.uniform('learning_rate', 0.01, 0.1)</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>hp.quniform</td>\n",
    "      <td>연속적이지만 일정 간격(q)을 갖는 값 샘플링</td>\n",
    "      <td>hp.quniform(label, low, high, q)</td>\n",
    "      <td><code>hp.quniform('num_layers', 1, 5, 1)</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>hp.loguniform</td>\n",
    "      <td>로그 스케일로 분포된 실수 값 샘플링</td>\n",
    "      <td>hp.loguniform(label, low, high)</td>\n",
    "      <td><code>hp.loguniform('reg_param', -3, 0)</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>hp.randint</td>\n",
    "      <td>정수 값 샘플링</td>\n",
    "      <td>hp.randint(label, upper)</td>\n",
    "      <td><code>hp.randint('num_trees', 1, 100)</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>hp.choice</td>\n",
    "      <td>주어진 리스트 중 임의의 값 샘플링</td>\n",
    "      <td>hp.choice(label, options)</td>\n",
    "      <td><code>hp.choice('optimizer', ['adam', 'sgd', 'rmsprop'])</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>hp.normal</td>\n",
    "      <td>정규분포에서 값 샘플링</td>\n",
    "      <td>hp.normal(label, mean, std)</td>\n",
    "      <td><code>hp.normal('dropout_rate', 0.3, 0.05)</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>hp.lognormal</td>\n",
    "      <td>로그 정규분포에서 값 샘플링</td>\n",
    "      <td>hp.lognormal(label, mean, std)</td>\n",
    "      <td><code>hp.lognormal('scale', 0, 1)</code></td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyperopt\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\hyuna\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from hyperopt) (2.2.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\hyuna\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from hyperopt) (1.15.2)\n",
      "Requirement already satisfied: six in c:\\users\\hyuna\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from hyperopt) (1.17.0)\n",
      "Collecting networkx>=2.2 (from hyperopt)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting future (from hyperopt)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting tqdm (from hyperopt)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting cloudpickle (from hyperopt)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting py4j (from hyperopt)\n",
      "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hyuna\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from tqdm->hyperopt) (0.4.6)\n",
      "Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 28.1 MB/s eta 0:00:00\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 31.1 MB/s eta 0:00:00\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: py4j, tqdm, networkx, future, cloudpickle, hyperopt\n",
      "Successfully installed cloudpickle-3.1.1 future-1.0.0 hyperopt-0.2.7 networkx-3.4.2 py4j-0.10.9.9 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperOpt의 핵심 개념\n",
    "- Search Space :어떤 파라미터를 어떤 범위에서 탐색할지 지정 (ex: 0.01 ~ 1.0)\n",
    "- Objective Function\t평가할 함수 → 모델을 학습시키고, 결과(score)를 반환\n",
    "- Trials\t시도 횟수 (몇 번 실험할지)\n",
    "- TPE (Tree-structured Parzen Estimator)\t똑똑한 탐색 알고리즘 (기본 알고리즘)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "# 검색 공간\n",
    "search_space = { #hp.quniform = 연속적이지만, 일정 간격(q)을 갖는 값 샘플링\n",
    "    'x': hp.quniform('x', -10, 10, 1), # hp.quniform(name, low, high, q) low에서 high까지, 간격 q로 나뉘어진 값을 정수처럼 반환\n",
    "    'y': hp.quniform('y', -15, 15, 1) # y도 마찬가지, 이중에서 x와 동일하게 랜덤하게 하나 선택\n",
    "}\n",
    "\n",
    "# 출력 예시 (실제로 뽑히는 값 형태)\n",
    "# {'x': 3.0, 'y': -12.0}\n",
    "# {'x': -7.0, 'y': 14.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "\n",
    "# 목적 함수\n",
    "def objective(search_space): #serch_space 하이퍼파라미터 조합 x,y를 받아서, 이 조합이 얼마나 좋은지 계산. \n",
    "    x = search_space['x'] # 예) 'x':3.0, \n",
    "    y = search_space['y'] # 예) 'y':1.0\n",
    "\n",
    "    return { #손실 계산\n",
    "        'loss': x**2 + 20 * y, # loss = x^2+20 *y -> HyperOpt는 이 loss 값을 최소화하려고 노력함. 즉, 값이 적을수록 좋은 조합이라고 판단. \n",
    "        'status': hyperopt.STATUS_OK # HyperOpt에게 → \"이 결과는 정상적으로 계산된 거야!\"라고 알려주는 역할\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:05<00:00, 87.94trial/s, best loss: -300.0] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x': np.float64(-0.0), 'y': np.float64(-15.0)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HyperOpt의 전체 최적화 과정을 실행하는 핵심 부분이에요.\n",
    "# 이제 우리가 만든 목적 함수와 탐색 공간을 가지고,\n",
    "# 최적의 하이퍼파라미터 값을 자동으로 찾아주는 역할을 합니다.\n",
    "from hyperopt import fmin, tpe, Trials \n",
    "# fmin 하이퍼 파라미터 최적화 시작 함수 (최솟값 찾기)\n",
    "# tpe: \"Tree-structured Parzen Estimator\" → 똑똑하게 탐색하는 알고리즘\n",
    "# Trials: 실험 결과(히스토리)를 저장해주는 객체\n",
    "\n",
    "# 탐색 과정을 저장하는 객체\n",
    "trials = Trials() # 지금까지 어떤 값으로 시도했는지, 결과는 어땠는지를 전부 기록하는 용도\n",
    "# 나중에 trials.results로 실험 로그를 확인할 수 있음.\n",
    "\n",
    "# fmin() : 목적 함수의 최소값을 찾는 함수\n",
    "best_val = fmin( # fmin()은 손실 함수(loss)가 가장 작아지는 x, y 값을 찾아주는 함수예요.\n",
    "    fn=objective,           # 목적함수, 손실 계산 함수\n",
    "    space=search_space,     # 검색공간,  x,y값의 범위 정의\n",
    "    algo=tpe.suggest,       # 베이지안 최적화 적용, 탐색 알고리즘(TPE 사용: 똑똑한 탐색)     \n",
    "    max_evals=500,          # 반복 횟수 (총 500번 실험)\n",
    "    trials=trials           # 탐색과정 저장 (결과 저장용)\n",
    ")\n",
    "\n",
    "best_val \n",
    "# {'x': np.float64(0.0), 'y': np.float64(-15.0)} 가 loss가 가장 적었던 x, y 조합."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 101.0, 'status': 'ok'},\n",
       " {'loss': -124.0, 'status': 'ok'},\n",
       " {'loss': 249.0, 'status': 'ok'},\n",
       " {'loss': -76.0, 'status': 'ok'},\n",
       " {'loss': -119.0, 'status': 'ok'},\n",
       " {'loss': 236.0, 'status': 'ok'},\n",
       " {'loss': 45.0, 'status': 'ok'},\n",
       " {'loss': 61.0, 'status': 'ok'},\n",
       " {'loss': -44.0, 'status': 'ok'},\n",
       " {'loss': 289.0, 'status': 'ok'},\n",
       " {'loss': -136.0, 'status': 'ok'},\n",
       " {'loss': 84.0, 'status': 'ok'},\n",
       " {'loss': -51.0, 'status': 'ok'},\n",
       " {'loss': -239.0, 'status': 'ok'},\n",
       " {'loss': 236.0, 'status': 'ok'},\n",
       " {'loss': 76.0, 'status': 'ok'},\n",
       " {'loss': -175.0, 'status': 'ok'},\n",
       " {'loss': 101.0, 'status': 'ok'},\n",
       " {'loss': 45.0, 'status': 'ok'},\n",
       " {'loss': -176.0, 'status': 'ok'},\n",
       " {'loss': -279.0, 'status': 'ok'},\n",
       " {'loss': -279.0, 'status': 'ok'},\n",
       " {'loss': -296.0, 'status': 'ok'},\n",
       " {'loss': -180.0, 'status': 'ok'},\n",
       " {'loss': -291.0, 'status': 'ok'},\n",
       " {'loss': -224.0, 'status': 'ok'},\n",
       " {'loss': 144.0, 'status': 'ok'},\n",
       " {'loss': 309.0, 'status': 'ok'},\n",
       " {'loss': -171.0, 'status': 'ok'},\n",
       " {'loss': -200.0, 'status': 'ok'},\n",
       " {'loss': -79.0, 'status': 'ok'},\n",
       " {'loss': -215.0, 'status': 'ok'},\n",
       " {'loss': -171.0, 'status': 'ok'},\n",
       " {'loss': -71.0, 'status': 'ok'},\n",
       " {'loss': -300.0, 'status': 'ok'},\n",
       " {'loss': -31.0, 'status': 'ok'},\n",
       " {'loss': -100.0, 'status': 'ok'},\n",
       " {'loss': -156.0, 'status': 'ok'},\n",
       " {'loss': -160.0, 'status': 'ok'},\n",
       " {'loss': -119.0, 'status': 'ok'},\n",
       " {'loss': 140.0, 'status': 'ok'},\n",
       " {'loss': 325.0, 'status': 'ok'},\n",
       " {'loss': -136.0, 'status': 'ok'},\n",
       " {'loss': 64.0, 'status': 'ok'},\n",
       " {'loss': 69.0, 'status': 'ok'},\n",
       " {'loss': -171.0, 'status': 'ok'},\n",
       " {'loss': 276.0, 'status': 'ok'},\n",
       " {'loss': -24.0, 'status': 'ok'},\n",
       " {'loss': -116.0, 'status': 'ok'},\n",
       " {'loss': -224.0, 'status': 'ok'},\n",
       " {'loss': -124.0, 'status': 'ok'},\n",
       " {'loss': 61.0, 'status': 'ok'},\n",
       " {'loss': 244.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -136.0, 'status': 'ok'},\n",
       " {'loss': -224.0, 'status': 'ok'},\n",
       " {'loss': -64.0, 'status': 'ok'},\n",
       " {'loss': -135.0, 'status': 'ok'},\n",
       " {'loss': -20.0, 'status': 'ok'},\n",
       " {'loss': -139.0, 'status': 'ok'},\n",
       " {'loss': -276.0, 'status': 'ok'},\n",
       " {'loss': 56.0, 'status': 'ok'},\n",
       " {'loss': 101.0, 'status': 'ok'},\n",
       " {'loss': -164.0, 'status': 'ok'},\n",
       " {'loss': -99.0, 'status': 'ok'},\n",
       " {'loss': -239.0, 'status': 'ok'},\n",
       " {'loss': -296.0, 'status': 'ok'},\n",
       " {'loss': -279.0, 'status': 'ok'},\n",
       " {'loss': -251.0, 'status': 'ok'},\n",
       " {'loss': -216.0, 'status': 'ok'},\n",
       " {'loss': -300.0, 'status': 'ok'},\n",
       " {'loss': -291.0, 'status': 'ok'},\n",
       " {'loss': -176.0, 'status': 'ok'},\n",
       " {'loss': -151.0, 'status': 'ok'},\n",
       " {'loss': -215.0, 'status': 'ok'},\n",
       " {'loss': -140.0, 'status': 'ok'},\n",
       " {'loss': -255.0, 'status': 'ok'},\n",
       " {'loss': -119.0, 'status': 'ok'},\n",
       " {'loss': -251.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': 276.0, 'status': 'ok'},\n",
       " {'loss': -171.0, 'status': 'ok'},\n",
       " {'loss': -59.0, 'status': 'ok'},\n",
       " {'loss': -140.0, 'status': 'ok'},\n",
       " {'loss': -100.0, 'status': 'ok'},\n",
       " {'loss': -216.0, 'status': 'ok'},\n",
       " {'loss': -199.0, 'status': 'ok'},\n",
       " {'loss': 216.0, 'status': 'ok'},\n",
       " {'loss': -236.0, 'status': 'ok'},\n",
       " {'loss': -151.0, 'status': 'ok'},\n",
       " {'loss': 104.0, 'status': 'ok'},\n",
       " {'loss': -235.0, 'status': 'ok'},\n",
       " {'loss': -131.0, 'status': 'ok'},\n",
       " {'loss': 0.0, 'status': 'ok'},\n",
       " {'loss': -11.0, 'status': 'ok'},\n",
       " {'loss': 156.0, 'status': 'ok'},\n",
       " {'loss': -264.0, 'status': 'ok'},\n",
       " {'loss': -219.0, 'status': 'ok'},\n",
       " {'loss': -184.0, 'status': 'ok'},\n",
       " {'loss': -156.0, 'status': 'ok'},\n",
       " {'loss': -231.0, 'status': 'ok'},\n",
       " {'loss': -39.0, 'status': 'ok'},\n",
       " {'loss': -296.0, 'status': 'ok'},\n",
       " {'loss': -235.0, 'status': 'ok'},\n",
       " {'loss': -119.0, 'status': 'ok'},\n",
       " {'loss': 220.0, 'status': 'ok'},\n",
       " {'loss': 76.0, 'status': 'ok'},\n",
       " {'loss': -271.0, 'status': 'ok'},\n",
       " {'loss': 44.0, 'status': 'ok'},\n",
       " {'loss': -151.0, 'status': 'ok'},\n",
       " {'loss': -64.0, 'status': 'ok'},\n",
       " {'loss': -179.0, 'status': 'ok'},\n",
       " {'loss': 289.0, 'status': 'ok'},\n",
       " {'loss': -115.0, 'status': 'ok'},\n",
       " {'loss': 84.0, 'status': 'ok'},\n",
       " {'loss': -159.0, 'status': 'ok'},\n",
       " {'loss': -91.0, 'status': 'ok'},\n",
       " {'loss': -276.0, 'status': 'ok'},\n",
       " {'loss': 241.0, 'status': 'ok'},\n",
       " {'loss': -300.0, 'status': 'ok'},\n",
       " {'loss': -164.0, 'status': 'ok'},\n",
       " {'loss': -216.0, 'status': 'ok'},\n",
       " {'loss': -135.0, 'status': 'ok'},\n",
       " {'loss': -240.0, 'status': 'ok'},\n",
       " {'loss': -279.0, 'status': 'ok'},\n",
       " {'loss': -264.0, 'status': 'ok'},\n",
       " {'loss': -224.0, 'status': 'ok'},\n",
       " {'loss': 36.0, 'status': 'ok'},\n",
       " {'loss': -140.0, 'status': 'ok'},\n",
       " {'loss': -76.0, 'status': 'ok'},\n",
       " {'loss': -259.0, 'status': 'ok'},\n",
       " {'loss': -219.0, 'status': 'ok'},\n",
       " {'loss': 21.0, 'status': 'ok'},\n",
       " {'loss': -191.0, 'status': 'ok'},\n",
       " {'loss': -104.0, 'status': 'ok'},\n",
       " {'loss': 236.0, 'status': 'ok'},\n",
       " {'loss': -251.0, 'status': 'ok'},\n",
       " {'loss': -219.0, 'status': 'ok'},\n",
       " {'loss': -75.0, 'status': 'ok'},\n",
       " {'loss': 24.0, 'status': 'ok'},\n",
       " {'loss': 89.0, 'status': 'ok'},\n",
       " {'loss': -16.0, 'status': 'ok'},\n",
       " {'loss': 44.0, 'status': 'ok'},\n",
       " {'loss': -280.0, 'status': 'ok'},\n",
       " {'loss': -155.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -279.0, 'status': 'ok'},\n",
       " {'loss': -236.0, 'status': 'ok'},\n",
       " {'loss': -259.0, 'status': 'ok'},\n",
       " {'loss': -231.0, 'status': 'ok'},\n",
       " {'loss': -300.0, 'status': 'ok'},\n",
       " {'loss': -291.0, 'status': 'ok'},\n",
       " {'loss': -260.0, 'status': 'ok'},\n",
       " {'loss': -196.0, 'status': 'ok'},\n",
       " {'loss': -276.0, 'status': 'ok'},\n",
       " {'loss': -204.0, 'status': 'ok'},\n",
       " {'loss': -159.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -240.0, 'status': 'ok'},\n",
       " {'loss': -251.0, 'status': 'ok'},\n",
       " {'loss': -199.0, 'status': 'ok'},\n",
       " {'loss': -276.0, 'status': 'ok'},\n",
       " {'loss': -240.0, 'status': 'ok'},\n",
       " {'loss': -176.0, 'status': 'ok'},\n",
       " {'loss': -219.0, 'status': 'ok'},\n",
       " {'loss': -131.0, 'status': 'ok'},\n",
       " {'loss': -284.0, 'status': 'ok'},\n",
       " {'loss': -251.0, 'status': 'ok'},\n",
       " {'loss': -124.0, 'status': 'ok'},\n",
       " {'loss': 145.0, 'status': 'ok'},\n",
       " {'loss': -116.0, 'status': 'ok'},\n",
       " {'loss': -195.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -280.0, 'status': 'ok'},\n",
       " {'loss': -199.0, 'status': 'ok'},\n",
       " {'loss': -236.0, 'status': 'ok'},\n",
       " {'loss': -256.0, 'status': 'ok'},\n",
       " {'loss': -179.0, 'status': 'ok'},\n",
       " {'loss': 276.0, 'status': 'ok'},\n",
       " {'loss': 209.0, 'status': 'ok'},\n",
       " {'loss': -244.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -251.0, 'status': 'ok'},\n",
       " {'loss': 0.0, 'status': 'ok'},\n",
       " {'loss': -196.0, 'status': 'ok'},\n",
       " {'loss': -91.0, 'status': 'ok'},\n",
       " {'loss': -279.0, 'status': 'ok'},\n",
       " {'loss': -240.0, 'status': 'ok'},\n",
       " {'loss': 149.0, 'status': 'ok'},\n",
       " {'loss': -171.0, 'status': 'ok'},\n",
       " {'loss': 296.0, 'status': 'ok'},\n",
       " {'loss': -296.0, 'status': 'ok'},\n",
       " {'loss': -216.0, 'status': 'ok'},\n",
       " {'loss': -59.0, 'status': 'ok'},\n",
       " {'loss': -104.0, 'status': 'ok'},\n",
       " {'loss': -235.0, 'status': 'ok'},\n",
       " {'loss': -19.0, 'status': 'ok'},\n",
       " {'loss': 29.0, 'status': 'ok'},\n",
       " {'loss': -135.0, 'status': 'ok'},\n",
       " {'loss': -219.0, 'status': 'ok'},\n",
       " {'loss': -280.0, 'status': 'ok'},\n",
       " {'loss': -236.0, 'status': 'ok'},\n",
       " {'loss': -284.0, 'status': 'ok'},\n",
       " {'loss': -84.0, 'status': 'ok'},\n",
       " {'loss': -176.0, 'status': 'ok'},\n",
       " {'loss': -279.0, 'status': 'ok'},\n",
       " {'loss': 84.0, 'status': 'ok'},\n",
       " {'loss': -231.0, 'status': 'ok'},\n",
       " {'loss': -199.0, 'status': 'ok'},\n",
       " {'loss': -264.0, 'status': 'ok'},\n",
       " {'loss': -111.0, 'status': 'ok'},\n",
       " {'loss': -80.0, 'status': 'ok'},\n",
       " {'loss': 76.0, 'status': 'ok'},\n",
       " {'loss': -235.0, 'status': 'ok'},\n",
       " {'loss': -184.0, 'status': 'ok'},\n",
       " {'loss': -255.0, 'status': 'ok'},\n",
       " {'loss': -191.0, 'status': 'ok'},\n",
       " {'loss': -260.0, 'status': 'ok'},\n",
       " {'loss': 161.0, 'status': 'ok'},\n",
       " {'loss': -236.0, 'status': 'ok'},\n",
       " {'loss': -131.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -136.0, 'status': 'ok'},\n",
       " {'loss': 249.0, 'status': 'ok'},\n",
       " {'loss': 169.0, 'status': 'ok'},\n",
       " {'loss': -276.0, 'status': 'ok'},\n",
       " {'loss': -259.0, 'status': 'ok'},\n",
       " {'loss': -111.0, 'status': 'ok'},\n",
       " {'loss': -220.0, 'status': 'ok'},\n",
       " {'loss': -176.0, 'status': 'ok'},\n",
       " {'loss': -79.0, 'status': 'ok'},\n",
       " {'loss': -284.0, 'status': 'ok'},\n",
       " {'loss': -176.0, 'status': 'ok'},\n",
       " {'loss': 29.0, 'status': 'ok'},\n",
       " {'loss': -220.0, 'status': 'ok'},\n",
       " {'loss': -256.0, 'status': 'ok'},\n",
       " {'loss': -140.0, 'status': 'ok'},\n",
       " {'loss': -84.0, 'status': 'ok'},\n",
       " {'loss': -59.0, 'status': 'ok'},\n",
       " {'loss': -39.0, 'status': 'ok'},\n",
       " {'loss': -279.0, 'status': 'ok'},\n",
       " {'loss': -296.0, 'status': 'ok'},\n",
       " {'loss': -135.0, 'status': 'ok'},\n",
       " {'loss': 200.0, 'status': 'ok'},\n",
       " {'loss': -239.0, 'status': 'ok'},\n",
       " {'loss': -251.0, 'status': 'ok'},\n",
       " {'loss': -264.0, 'status': 'ok'},\n",
       " {'loss': -219.0, 'status': 'ok'},\n",
       " {'loss': -196.0, 'status': 'ok'},\n",
       " {'loss': -251.0, 'status': 'ok'},\n",
       " {'loss': 205.0, 'status': 'ok'},\n",
       " {'loss': -224.0, 'status': 'ok'},\n",
       " {'loss': -104.0, 'status': 'ok'},\n",
       " {'loss': -199.0, 'status': 'ok'},\n",
       " {'loss': -276.0, 'status': 'ok'},\n",
       " {'loss': 104.0, 'status': 'ok'},\n",
       " {'loss': -240.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -256.0, 'status': 'ok'},\n",
       " {'loss': -280.0, 'status': 'ok'},\n",
       " {'loss': -211.0, 'status': 'ok'},\n",
       " {'loss': -240.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -279.0, 'status': 'ok'},\n",
       " {'loss': -256.0, 'status': 'ok'},\n",
       " {'loss': -271.0, 'status': 'ok'},\n",
       " {'loss': -176.0, 'status': 'ok'},\n",
       " {'loss': -296.0, 'status': 'ok'},\n",
       " {'loss': -259.0, 'status': 'ok'},\n",
       " {'loss': -291.0, 'status': 'ok'},\n",
       " {'loss': -204.0, 'status': 'ok'},\n",
       " {'loss': -239.0, 'status': 'ok'},\n",
       " {'loss': -276.0, 'status': 'ok'},\n",
       " {'loss': -240.0, 'status': 'ok'},\n",
       " {'loss': -219.0, 'status': 'ok'},\n",
       " {'loss': -151.0, 'status': 'ok'},\n",
       " {'loss': -276.0, 'status': 'ok'},\n",
       " {'loss': -260.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -191.0, 'status': 'ok'},\n",
       " {'loss': -231.0, 'status': 'ok'},\n",
       " {'loss': -244.0, 'status': 'ok'},\n",
       " {'loss': -196.0, 'status': 'ok'},\n",
       " {'loss': -291.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -279.0, 'status': 'ok'},\n",
       " {'loss': -240.0, 'status': 'ok'},\n",
       " {'loss': -176.0, 'status': 'ok'},\n",
       " {'loss': -204.0, 'status': 'ok'},\n",
       " {'loss': 325.0, 'status': 'ok'},\n",
       " {'loss': 161.0, 'status': 'ok'},\n",
       " {'loss': -260.0, 'status': 'ok'},\n",
       " {'loss': -159.0, 'status': 'ok'},\n",
       " {'loss': -276.0, 'status': 'ok'},\n",
       " {'loss': 69.0, 'status': 'ok'},\n",
       " {'loss': -236.0, 'status': 'ok'},\n",
       " {'loss': -280.0, 'status': 'ok'},\n",
       " {'loss': 1.0, 'status': 'ok'},\n",
       " {'loss': -259.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -60.0, 'status': 'ok'},\n",
       " {'loss': -124.0, 'status': 'ok'},\n",
       " {'loss': -175.0, 'status': 'ok'},\n",
       " {'loss': -216.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -171.0, 'status': 'ok'},\n",
       " {'loss': -279.0, 'status': 'ok'},\n",
       " {'loss': 224.0, 'status': 'ok'},\n",
       " {'loss': -215.0, 'status': 'ok'},\n",
       " {'loss': 104.0, 'status': 'ok'},\n",
       " {'loss': -251.0, 'status': 'ok'},\n",
       " {'loss': -284.0, 'status': 'ok'},\n",
       " {'loss': -219.0, 'status': 'ok'},\n",
       " {'loss': -20.0, 'status': 'ok'},\n",
       " {'loss': -196.0, 'status': 'ok'},\n",
       " {'loss': -279.0, 'status': 'ok'},\n",
       " {'loss': -4.0, 'status': 'ok'},\n",
       " {'loss': -179.0, 'status': 'ok'},\n",
       " {'loss': -140.0, 'status': 'ok'},\n",
       " {'loss': -91.0, 'status': 'ok'},\n",
       " {'loss': -159.0, 'status': 'ok'},\n",
       " {'loss': -256.0, 'status': 'ok'},\n",
       " {'loss': -184.0, 'status': 'ok'},\n",
       " {'loss': 240.0, 'status': 'ok'},\n",
       " {'loss': -291.0, 'status': 'ok'},\n",
       " {'loss': -256.0, 'status': 'ok'},\n",
       " {'loss': -300.0, 'status': 'ok'},\n",
       " {'loss': -276.0, 'status': 'ok'},\n",
       " {'loss': -280.0, 'status': 'ok'},\n",
       " {'loss': -131.0, 'status': 'ok'},\n",
       " {'loss': -224.0, 'status': 'ok'},\n",
       " {'loss': 281.0, 'status': 'ok'},\n",
       " {'loss': -199.0, 'status': 'ok'},\n",
       " {'loss': 165.0, 'status': 'ok'},\n",
       " {'loss': -300.0, 'status': 'ok'},\n",
       " {'loss': 96.0, 'status': 'ok'},\n",
       " {'loss': -211.0, 'status': 'ok'},\n",
       " {'loss': -256.0, 'status': 'ok'},\n",
       " {'loss': -79.0, 'status': 'ok'},\n",
       " {'loss': -191.0, 'status': 'ok'},\n",
       " {'loss': -156.0, 'status': 'ok'},\n",
       " {'loss': -119.0, 'status': 'ok'},\n",
       " {'loss': -264.0, 'status': 'ok'},\n",
       " {'loss': -240.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -260.0, 'status': 'ok'},\n",
       " {'loss': -144.0, 'status': 'ok'},\n",
       " {'loss': -216.0, 'status': 'ok'},\n",
       " {'loss': -264.0, 'status': 'ok'},\n",
       " {'loss': -176.0, 'status': 'ok'},\n",
       " {'loss': -236.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -140.0, 'status': 'ok'},\n",
       " {'loss': -291.0, 'status': 'ok'},\n",
       " {'loss': 201.0, 'status': 'ok'},\n",
       " {'loss': -184.0, 'status': 'ok'},\n",
       " {'loss': -251.0, 'status': 'ok'},\n",
       " {'loss': 124.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -280.0, 'status': 'ok'},\n",
       " {'loss': -220.0, 'status': 'ok'},\n",
       " {'loss': -211.0, 'status': 'ok'},\n",
       " {'loss': -255.0, 'status': 'ok'},\n",
       " {'loss': -219.0, 'status': 'ok'},\n",
       " {'loss': -236.0, 'status': 'ok'},\n",
       " {'loss': -155.0, 'status': 'ok'},\n",
       " {'loss': -259.0, 'status': 'ok'},\n",
       " {'loss': -151.0, 'status': 'ok'},\n",
       " {'loss': -284.0, 'status': 'ok'},\n",
       " {'loss': 16.0, 'status': 'ok'},\n",
       " {'loss': -236.0, 'status': 'ok'},\n",
       " {'loss': 181.0, 'status': 'ok'},\n",
       " {'loss': -271.0, 'status': 'ok'},\n",
       " {'loss': -59.0, 'status': 'ok'},\n",
       " {'loss': -96.0, 'status': 'ok'},\n",
       " {'loss': -116.0, 'status': 'ok'},\n",
       " {'loss': -119.0, 'status': 'ok'},\n",
       " {'loss': -271.0, 'status': 'ok'},\n",
       " {'loss': -240.0, 'status': 'ok'},\n",
       " {'loss': -195.0, 'status': 'ok'},\n",
       " {'loss': -291.0, 'status': 'ok'},\n",
       " {'loss': -256.0, 'status': 'ok'},\n",
       " {'loss': 41.0, 'status': 'ok'},\n",
       " {'loss': -259.0, 'status': 'ok'},\n",
       " {'loss': 285.0, 'status': 'ok'},\n",
       " {'loss': -300.0, 'status': 'ok'},\n",
       " {'loss': 24.0, 'status': 'ok'},\n",
       " {'loss': -200.0, 'status': 'ok'},\n",
       " {'loss': 136.0, 'status': 'ok'},\n",
       " {'loss': -159.0, 'status': 'ok'},\n",
       " {'loss': -204.0, 'status': 'ok'},\n",
       " {'loss': -131.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': 316.0, 'status': 'ok'},\n",
       " {'loss': -264.0, 'status': 'ok'},\n",
       " {'loss': -180.0, 'status': 'ok'},\n",
       " {'loss': -216.0, 'status': 'ok'},\n",
       " {'loss': -259.0, 'status': 'ok'},\n",
       " {'loss': -16.0, 'status': 'ok'},\n",
       " {'loss': -271.0, 'status': 'ok'},\n",
       " {'loss': -300.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -280.0, 'status': 'ok'},\n",
       " {'loss': -259.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -260.0, 'status': 'ok'},\n",
       " {'loss': -276.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -240.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -276.0, 'status': 'ok'},\n",
       " {'loss': -239.0, 'status': 'ok'},\n",
       " {'loss': -276.0, 'status': 'ok'},\n",
       " {'loss': -239.0, 'status': 'ok'},\n",
       " {'loss': -300.0, 'status': 'ok'},\n",
       " {'loss': -256.0, 'status': 'ok'},\n",
       " {'loss': -279.0, 'status': 'ok'},\n",
       " {'loss': -259.0, 'status': 'ok'},\n",
       " {'loss': -216.0, 'status': 'ok'},\n",
       " {'loss': -300.0, 'status': 'ok'},\n",
       " {'loss': -300.0, 'status': 'ok'},\n",
       " {'loss': -291.0, 'status': 'ok'},\n",
       " {'loss': -240.0, 'status': 'ok'},\n",
       " {'loss': -199.0, 'status': 'ok'},\n",
       " {'loss': -211.0, 'status': 'ok'},\n",
       " {'loss': -256.0, 'status': 'ok'},\n",
       " {'loss': -279.0, 'status': 'ok'},\n",
       " {'loss': -256.0, 'status': 'ok'},\n",
       " {'loss': -236.0, 'status': 'ok'},\n",
       " {'loss': -279.0, 'status': 'ok'},\n",
       " {'loss': -211.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -191.0, 'status': 'ok'},\n",
       " {'loss': -280.0, 'status': 'ok'},\n",
       " {'loss': -260.0, 'status': 'ok'},\n",
       " {'loss': -236.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -219.0, 'status': 'ok'},\n",
       " {'loss': -296.0, 'status': 'ok'},\n",
       " {'loss': -260.0, 'status': 'ok'},\n",
       " {'loss': -271.0, 'status': 'ok'},\n",
       " {'loss': -300.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': 164.0, 'status': 'ok'},\n",
       " {'loss': -179.0, 'status': 'ok'},\n",
       " {'loss': -191.0, 'status': 'ok'},\n",
       " {'loss': -240.0, 'status': 'ok'},\n",
       " {'loss': -264.0, 'status': 'ok'},\n",
       " {'loss': -259.0, 'status': 'ok'},\n",
       " {'loss': -219.0, 'status': 'ok'},\n",
       " {'loss': -236.0, 'status': 'ok'},\n",
       " {'loss': -176.0, 'status': 'ok'},\n",
       " {'loss': -24.0, 'status': 'ok'},\n",
       " {'loss': -279.0, 'status': 'ok'},\n",
       " {'loss': -251.0, 'status': 'ok'},\n",
       " {'loss': -296.0, 'status': 'ok'},\n",
       " {'loss': -199.0, 'status': 'ok'},\n",
       " {'loss': -276.0, 'status': 'ok'},\n",
       " {'loss': -231.0, 'status': 'ok'},\n",
       " {'loss': 60.0, 'status': 'ok'},\n",
       " {'loss': -160.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -219.0, 'status': 'ok'},\n",
       " {'loss': -256.0, 'status': 'ok'},\n",
       " {'loss': -264.0, 'status': 'ok'},\n",
       " {'loss': -184.0, 'status': 'ok'},\n",
       " {'loss': -259.0, 'status': 'ok'},\n",
       " {'loss': -220.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -231.0, 'status': 'ok'},\n",
       " {'loss': -276.0, 'status': 'ok'},\n",
       " {'loss': 141.0, 'status': 'ok'},\n",
       " {'loss': -300.0, 'status': 'ok'},\n",
       " {'loss': -111.0, 'status': 'ok'},\n",
       " {'loss': 44.0, 'status': 'ok'},\n",
       " {'loss': -239.0, 'status': 'ok'},\n",
       " {'loss': -271.0, 'status': 'ok'},\n",
       " {'loss': -256.0, 'status': 'ok'},\n",
       " {'loss': -156.0, 'status': 'ok'},\n",
       " {'loss': 236.0, 'status': 'ok'},\n",
       " {'loss': -179.0, 'status': 'ok'},\n",
       " {'loss': -259.0, 'status': 'ok'},\n",
       " {'loss': -211.0, 'status': 'ok'},\n",
       " {'loss': -140.0, 'status': 'ok'},\n",
       " {'loss': -271.0, 'status': 'ok'},\n",
       " {'loss': -196.0, 'status': 'ok'},\n",
       " {'loss': -275.0, 'status': 'ok'},\n",
       " {'loss': -300.0, 'status': 'ok'},\n",
       " {'loss': -299.0, 'status': 'ok'},\n",
       " {'loss': -224.0, 'status': 'ok'},\n",
       " {'loss': -16.0, 'status': 'ok'},\n",
       " {'loss': -276.0, 'status': 'ok'},\n",
       " {'loss': -219.0, 'status': 'ok'},\n",
       " {'loss': -260.0, 'status': 'ok'},\n",
       " {'loss': -236.0, 'status': 'ok'},\n",
       " {'loss': -276.0, 'status': 'ok'},\n",
       " {'loss': -259.0, 'status': 'ok'},\n",
       " {'loss': -191.0, 'status': 'ok'},\n",
       " {'loss': -75.0, 'status': 'ok'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 탐색과정 -> 목적함수 반환값 (loss와 실행 상태) 저장 / 실험 로그 확인 \n",
    "trials.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': [np.float64(9.0),\n",
       "  np.float64(6.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(8.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(9.0),\n",
       "  np.float64(6.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(7.0),\n",
       "  np.float64(10.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(7.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(9.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(7.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(6.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(8.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(10.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(6.0),\n",
       "  np.float64(8.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(6.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(8.0),\n",
       "  np.float64(7.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(9.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(6.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(7.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(6.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(6.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(7.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(9.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(8.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(6.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(10.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(6.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(7.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(6.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(8.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-5.0)],\n",
       " 'y': [np.float64(1.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(10.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(11.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(12.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(10.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(7.0),\n",
       "  np.float64(15.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(7.0),\n",
       "  np.float64(15.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(13.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(9.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(13.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(9.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(6.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(11.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(14.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(8.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(11.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(6.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(13.0),\n",
       "  np.float64(10.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(7.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(14.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(8.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(12.0),\n",
       "  np.float64(6.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(10.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(9.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(15.0),\n",
       "  np.float64(8.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-0.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(11.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(12.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(14.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(7.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(4.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(10.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(6.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(0.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(9.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-3.0),\n",
       "  np.float64(-5.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(13.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(1.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(5.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(15.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-1.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(8.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-2.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(3.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(7.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-6.0),\n",
       "  np.float64(2.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-8.0),\n",
       "  np.float64(11.0),\n",
       "  np.float64(-9.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-7.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-15.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-4.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-11.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-12.0),\n",
       "  np.float64(-14.0),\n",
       "  np.float64(-13.0),\n",
       "  np.float64(-10.0),\n",
       "  np.float64(-5.0)]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 탐색과정 -> 하이퍼 파라미터값을 딕셔너리(리스트) 형태로 저장\n",
    "trials.vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.0-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\hyuna\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from xgboost) (2.2.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\hyuna\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from xgboost) (1.15.2)\n",
      "Downloading xgboost-3.0.0-py3-none-win_amd64.whl (150.0 MB)\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 7.6/150.0 MB 39.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 16.5/150.0 MB 40.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 23.6/150.0 MB 37.3 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 32.2/150.0 MB 37.9 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 40.6/150.0 MB 38.6 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 49.3/150.0 MB 38.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 58.5/150.0 MB 39.2 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 67.6/150.0 MB 39.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 77.1/150.0 MB 40.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 86.5/150.0 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 94.9/150.0 MB 40.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 103.3/150.0 MB 40.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 111.7/150.0 MB 40.3 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 120.6/150.0 MB 40.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 129.5/150.0 MB 40.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 138.7/150.0 MB 40.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  147.8/150.0 MB 40.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.9/150.0 MB 40.8 MB/s eta 0:00:01\n",
      "   --------------------------------------- 150.0/150.0 MB 38.5 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hyperopt를 활용한 XGBoost 하이퍼 파라미터 튜닝\n",
    "\n",
    "- XGBoost 란? \"여러 개의 약한 모델(결정트리)을 합쳐서 점점 더 좋은 예측을 만들어내는 똑똑한 머신러닝 알고리즘\"\n",
    "- XGBoost\teXtreme Gradient Boosting\n",
    "- Boosting\t성능이 낮은 모델(약한 모델)을 여러 개 모아서 점점 더 좋은 모델로 만드는 방식\n",
    "- Gradient\t모델이 얼마나 틀렸는지(오차)를 바탕으로 개선 방향을 찾는 방법\n",
    "\n",
    "- 예시로, 케익을 먹는데, 첫 번째 사람이 대충 맛 평가 (70) -> 두 번째 사람이 단것같다고해서 단맛 수정 -> 세 번째 사람의 또 다른 의견으로 조정... 이렇게 점점 평가를 보완하면서 최종점수를 만드는데 이걸 매우 빠르고 정확하게 구현한게 XGBoost\n",
    "\n",
    "- XGBoost는 성능이 낮은 트리들을 반복적으로 보완해가며높은 정확도를 만들어내는, 빠르고 똑똑한 머신러닝 알고리즘이다! 🌳⚡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ✅ XGBoost의 특징\n",
    "- 🎯 정확도 높음\t대부분의 캐글 대회 1등들이 자주 쓰는 이유\n",
    "- ⚡ 빠름\t일반적인 boosting보다 훨씬 속도 빠름 (병렬 처리, 캐시 최적화 등)\n",
    "- 🌳 트리 기반\t여러 개의 결정 트리(decision tree)를 만들어서 예측\n",
    "- 🔥 과적합 방지\t규제(regularization) 기능 내장 → 너무 복잡해지는 걸 막아줌\n",
    "- ✅ 범주형/수치형 둘 다 처리 잘함\t숫자, 범주형 데이터 모두 잘 다룸\n",
    "- 📊 분류/회귀/랭킹 모두 가능\tclassification, regression, ranking 문제에 다 사용 가능\n",
    "\n",
    "- 💡 어디에 쓰일까?\n",
    "- 고객 이탈 예측\n",
    "- 암 진단 (유방암, 당뇨 등)\n",
    "- 금융 사기 탐지\n",
    "- 가격 예측\n",
    "- 캐글(Kaggle) 데이터 대회 우승 모델 ✨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:22<00:00,  4.48trial/s, best loss: -0.971830985915493]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': np.float64(0.8012544657693954),\n",
       " 'learning_rate': np.float64(0.17929552306220722),\n",
       " 'max_depth': np.float64(6.0),\n",
       " 'n_estimators': np.float64(500.0)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 유방암 데이터를 이용해서 XGBoost 분류 모델을 만들고,하이퍼파라미터를 HyperOpt를 이용해서 자동으로 최적화하는 코드 \n",
    "\n",
    "# 📦 코드 구조는 3단계\n",
    "# 1️⃣ 탐색할 하이퍼파라미터 공간 정의 (search_space)\n",
    "# 2️⃣ \"이 조합이 얼마나 좋은지\" 평가하는 함수 작성 (objective 함수)\n",
    "# 3️⃣ HyperOpt로 최적값 자동으로 찾기 (fmin + Trials)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(data.data, data.target, random_state=42)\n",
    "\n",
    "# 1. 검색 공간\n",
    "search_space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 500, 100), # 트리 개수:  100~500 사이에서 100 간격으로 탐색 \n",
    "    'max_depth': hp.quniform('max_depth', 3, 10, 1), # 트리 깊이: 3~10 사이의 정수 값 탐색 \n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2), # 학습률: 0.01 ~ 0.2 사이의 실수 탐색\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1) # 컬럼 샘플링 비율: 전체 특징 중 몇 퍼센트만 쓸지 (0.5 ~ 1.0)\n",
    "}\n",
    "\n",
    "# 2. 목적 함수 / 이 함수는 HyperOpt가 시도해볼 하이퍼파라미터 조합 ss를 받아서, 그 조합으로 모델을 학습시키고, 정확도를 평가한 후 \"loss\" 값을 반환합니다.\n",
    "def xgb_objective(ss):\n",
    "\n",
    "    xgb_clf = XGBClassifier( # 실제 모델 생성 부분.\n",
    "        n_estimators=int(ss['n_estimators']), # quniform()은 실수로 값을 주기 때문에 int()로 변환해줘야함.\n",
    "        max_depth=int(ss['max_depth']),\n",
    "        learning_rate=ss['learning_rate'],\n",
    "        colsample_bytree=ss['colsample_bytree']\n",
    "    )\n",
    "    mean_acc = cross_val_score(xgb_clf, X_train, y_train, scoring='accuracy', cv=3).mean() # 3-fold 교차 검증으로 모델의 평균 정확도(accuracy)를 구합니다.\n",
    "    return {\n",
    "        'loss': -1 * mean_acc,# HyperOpt는 \"작은 값\"을 최적이라고 생각함 → 그래서 음수로 바꿈 /정확도가 클수록 좋은데, HyperOpt는 \"loss를 최소화\"하려고 하므로 정확도에 -1을 곱함\n",
    "        'status': hyperopt.STATUS_OK\n",
    "    }\n",
    "\n",
    "\n",
    "# 3. Trials() + fmin()\n",
    "trials = Trials() # 모든 실험 기록 \n",
    "best = fmin(  \n",
    "    fn=xgb_objective, # 목적 함수\n",
    "    space=search_space, # 하이퍼파라미터 범위\n",
    "    algo=tpe.suggest, # TPE 알고리즘 사용 (똑똑한 탐색)\n",
    "    #max_evals=50, # 총 50번 실험\n",
    "    max_evals=100, \n",
    "    trials=trials # 실험 로그 저장 \n",
    ")\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna\n",
    "- HyperOpt 이후로 나온 더 발전된 하이퍼 파라미터 최적화 프레임 워크\n",
    "- 가장 좋은 하이퍼파라미터 조합을 자동으로 찾아주는 똑똑한 도구 (하이퍼파라미터 튜닝 라이브러리)\n",
    "\n",
    "- ✅ 그럼 기존 HyperOpt랑 뭐가 달라?\n",
    "- 비교 항목\t   /     HyperOpt\t                /              Optuna\n",
    "- 코드 작성\t   /     search_space, objective 따로/\t하나의 함수 안에서 간단하게 작성 가능\n",
    "- 속도\t     /      빠름\t      /                      더 빠르고 똑똑한 알고리즘 (TPE 개선됨)\n",
    "- 사용성\t  /     좋아요\t /                       더 직관적이고, 심플함\n",
    "- 시각화\t  /     없음 (직접 구현해야 함)\t  /        자동 시각화 내장! (optuna.visualization)\n",
    "- 실험 저장\t   /    Trials로 수동 저장\t   /          자동 저장 가능 + DB 연동도 쉬움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>함수명</th>\n",
    "            <th>설명</th>\n",
    "            <th>사용 방법</th>\n",
    "            <th>예시 코드</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>suggest_uniform</td>\n",
    "            <td>연속적인 실수 값 샘플링</td>\n",
    "            <td>trial.suggest_uniform(name, low, high)</td>\n",
    "            <td><code>trial.suggest_uniform('learning_rate', 0.01, 0.1)</code></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>suggest_discrete_uniform</td>\n",
    "            <td>연속적이지만 일정 간격(step)을 갖는 값 샘플링</td>\n",
    "            <td>trial.suggest_discrete_uniform(name, low, high, step)</td>\n",
    "            <td><code>trial.suggest_discrete_uniform('num_layers', 1, 5, 1)</code></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>suggest_loguniform</td>\n",
    "            <td>로그 스케일로 분포된 실수 값 샘플링</td>\n",
    "            <td>trial.suggest_loguniform(name, low, high)</td>\n",
    "            <td><code>trial.suggest_loguniform('reg_param', 1e-3, 1)</code></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>suggest_int</td>\n",
    "            <td>정수 값 샘플링</td>\n",
    "            <td>trial.suggest_int(name, low, high, step)</td>\n",
    "            <td><code>trial.suggest_int('num_trees', 1, 100)</code></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>suggest_categorical</td>\n",
    "            <td>주어진 리스트 중 임의의 값 샘플링</td>\n",
    "            <td>trial.suggest_categorical(name, choices)</td>\n",
    "            <td><code>trial.suggest_categorical('optimizer', ['adam', 'sgd', 'rmsprop'])</code></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>suggest_float</td>\n",
    "            <td>연속적인 실수 값 샘플링 (<code>step</code> 사용 가능)</td>\n",
    "            <td>trial.suggest_float(name, low, high, step=None, log=False)</td>\n",
    "            <td><code>trial.suggest_float('alpha', 0.1, 1.0, step=0.1)</code></td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\hyuna\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from optuna) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hyuna\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from optuna) (24.2)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
      "  Downloading sqlalchemy-2.0.40-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hyuna\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\hyuna\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\hyuna\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.13.1)\n",
      "Collecting greenlet>=1 (from sqlalchemy>=1.4.2->optuna)\n",
      "  Downloading greenlet-3.1.1-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hyuna\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\hyuna\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
      "Downloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
      "Downloading sqlalchemy-2.0.40-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 39.3 MB/s eta 0:00:00\n",
      "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading greenlet-3.1.1-cp312-cp312-win_amd64.whl (299 kB)\n",
      "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: Mako, greenlet, colorlog, sqlalchemy, alembic, optuna\n",
      "Successfully installed Mako-1.3.9 alembic-1.15.2 colorlog-6.9.0 greenlet-3.1.1 optuna-4.2.1 sqlalchemy-2.0.40\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-09 17:33:17,619] A new study created in memory with name: no-name-6b5b7ae1-0861-451d-83b6-999dc8aa0353\n",
      "C:\\Users\\hyuna\\AppData\\Local\\Temp\\ipykernel_18888\\3508545321.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  x = trial.suggest_uniform('x', -10, 10)\n",
      "C:\\Users\\hyuna\\AppData\\Local\\Temp\\ipykernel_18888\\3508545321.py:6: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  y = trial.suggest_uniform('y', -15, 15)\n",
      "[I 2025-04-09 17:33:17,621] Trial 0 finished with value: 2.9785798178529217 and parameters: {'x': 3.514329743007277, 'y': -3.3525641944794966}. Best is trial 0 with value: 2.9785798178529217.\n",
      "[I 2025-04-09 17:33:17,622] Trial 1 finished with value: 14.137540595346495 and parameters: {'x': 6.09323011494595, 'y': -2.862368588521875}. Best is trial 0 with value: 2.9785798178529217.\n",
      "[I 2025-04-09 17:33:17,623] Trial 2 finished with value: 45.75110163881073 and parameters: {'x': 9.291300514698115, 'y': -7.484077187321273}. Best is trial 0 with value: 2.9785798178529217.\n",
      "[I 2025-04-09 17:33:17,623] Trial 3 finished with value: 143.51631138837243 and parameters: {'x': 4.970926084384391, 'y': 6.816588414524119}. Best is trial 0 with value: 2.9785798178529217.\n",
      "[I 2025-04-09 17:33:17,624] Trial 4 finished with value: 31.934548259980524 and parameters: {'x': 4.996971231671193, 'y': 0.28645951085016463}. Best is trial 0 with value: 2.9785798178529217.\n",
      "[I 2025-04-09 17:33:17,625] Trial 5 finished with value: 399.85299037482014 and parameters: {'x': 9.833047236457965, 'y': 13.792617056683572}. Best is trial 0 with value: 2.9785798178529217.\n",
      "[I 2025-04-09 17:33:17,626] Trial 6 finished with value: 162.52168636077624 and parameters: {'x': 0.27321798577080614, 'y': 7.45336686232492}. Best is trial 0 with value: 2.9785798178529217.\n",
      "[I 2025-04-09 17:33:17,626] Trial 7 finished with value: 37.32226091120133 and parameters: {'x': -1.8886374507989245, 'y': -1.3361925834116875}. Best is trial 0 with value: 2.9785798178529217.\n",
      "[I 2025-04-09 17:33:17,627] Trial 8 finished with value: 42.01869444685399 and parameters: {'x': 5.9822960542892645, 'y': -10.75539787429372}. Best is trial 0 with value: 2.9785798178529217.\n",
      "[I 2025-04-09 17:33:17,628] Trial 9 finished with value: 159.74832726902326 and parameters: {'x': -4.83514578048573, 'y': 4.917601417054424}. Best is trial 0 with value: 2.9785798178529217.\n",
      "[I 2025-04-09 17:33:17,635] Trial 10 finished with value: 211.8237814274798 and parameters: {'x': -8.033617064497026, 'y': -14.491210455022047}. Best is trial 0 with value: 2.9785798178529217.\n",
      "[I 2025-04-09 17:33:17,640] Trial 11 finished with value: 2.04703573939661 and parameters: {'x': 1.825693269595691, 'y': -4.182663201413425}. Best is trial 11 with value: 2.04703573939661.\n",
      "[I 2025-04-09 17:33:17,645] Trial 12 finished with value: 6.269432851577984 and parameters: {'x': 0.7896134703380242, 'y': -6.176275580409145}. Best is trial 11 with value: 2.04703573939661.\n",
      "[I 2025-04-09 17:33:17,651] Trial 13 finished with value: 43.67015610533845 and parameters: {'x': 2.0007372011096383, 'y': 1.5323525596902874}. Best is trial 11 with value: 2.04703573939661.\n",
      "[I 2025-04-09 17:33:17,656] Trial 14 finished with value: 39.64444039777808 and parameters: {'x': -3.271178163646045, 'y': -5.5628186542636024}. Best is trial 11 with value: 2.04703573939661.\n",
      "[I 2025-04-09 17:33:17,661] Trial 15 finished with value: 35.744710210528204 and parameters: {'x': 2.55599389930396, 'y': -10.962178191992663}. Best is trial 11 with value: 2.04703573939661.\n",
      "[I 2025-04-09 17:33:17,666] Trial 16 finished with value: 58.646979340934855 and parameters: {'x': 2.9651494828919835, 'y': 2.6580522838638387}. Best is trial 11 with value: 2.04703573939661.\n",
      "[I 2025-04-09 17:33:17,672] Trial 17 finished with value: 19.146890733829963 and parameters: {'x': -1.1496936646037048, 'y': -3.61185979678642}. Best is trial 11 with value: 2.04703573939661.\n",
      "[I 2025-04-09 17:33:17,676] Trial 18 finished with value: 93.03990582907734 and parameters: {'x': -5.64911455290359, 'y': -9.269979306721364}. Best is trial 11 with value: 2.04703573939661.\n",
      "[I 2025-04-09 17:33:17,681] Trial 19 finished with value: 94.56946194453315 and parameters: {'x': 7.470412920554757, 'y': -13.636253253828897}. Best is trial 11 with value: 2.04703573939661.\n",
      "[I 2025-04-09 17:33:17,687] Trial 20 finished with value: 277.6573232676379 and parameters: {'x': 3.2836142617627906, 'y': 11.660638829833704}. Best is trial 11 with value: 2.04703573939661.\n",
      "[I 2025-04-09 17:33:17,692] Trial 21 finished with value: 7.3347336743013685 and parameters: {'x': 0.39144314705907135, 'y': -5.728124178472523}. Best is trial 11 with value: 2.04703573939661.\n",
      "[I 2025-04-09 17:33:17,697] Trial 22 finished with value: 4.257302627516394 and parameters: {'x': 0.9623843799404339, 'y': -4.6753075110113205}. Best is trial 11 with value: 2.04703573939661.\n",
      "[I 2025-04-09 17:33:17,703] Trial 23 finished with value: 27.730924862459496 and parameters: {'x': -1.6470246678812928, 'y': -2.522887447336666}. Best is trial 11 with value: 2.04703573939661.\n",
      "[I 2025-04-09 17:33:17,708] Trial 24 finished with value: 0.9372281820877353 and parameters: {'x': 3.743746715634113, 'y': -4.38026698888053}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,714] Trial 25 finished with value: 18.06168398744908 and parameters: {'x': 4.130598307959501, 'y': -0.9032413479083012}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,720] Trial 26 finished with value: 27.271655114122364 and parameters: {'x': 7.2207584400525215, 'y': -8.075199717879762}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,725] Trial 27 finished with value: 71.73445537138352 and parameters: {'x': 1.6684548273381714, 'y': 3.3642957159909463}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,733] Trial 28 finished with value: 28.187751674569725 and parameters: {'x': 3.8826154815230365, 'y': 0.23533586184741218}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,740] Trial 29 finished with value: 23.88214463446241 and parameters: {'x': 7.094490762172459, 'y': -2.332175037048569}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,745] Trial 30 finished with value: 12.876857110942693 and parameters: {'x': 6.086197730732517, 'y': -3.169087474027184}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,752] Trial 31 finished with value: 2.963082296558616 and parameters: {'x': 1.3099548153216123, 'y': -4.673152068533377}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,757] Trial 32 finished with value: 21.010105038144797 and parameters: {'x': -1.0445144492511216, 'y': -7.156851387542428}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,763] Trial 33 finished with value: 5.45512364061155 and parameters: {'x': 5.204200483624825, 'y': -4.22758568850672}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,769] Trial 34 finished with value: 13.23112809315232 and parameters: {'x': 1.7833810784070978, 'y': -8.427968274761954}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,773] Trial 35 finished with value: 57.904215442041675 and parameters: {'x': 8.253798064441062, 'y': -10.504709015026773}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,782] Trial 36 finished with value: 35.69060115344474 and parameters: {'x': 4.597831245091529, 'y': 0.7565212121257742}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,788] Trial 37 finished with value: 26.59866952595449 and parameters: {'x': -0.5034944743751781, 'y': -1.2152680419906767}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,793] Trial 38 finished with value: 34.87943223762687 and parameters: {'x': -2.5852805766019866, 'y': -6.919393945561842}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,798] Trial 39 finished with value: 157.7984676141817 and parameters: {'x': 2.8754633238034653, 'y': 7.561168664995454}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,803] Trial 40 finished with value: 5.288349878402421 and parameters: {'x': 5.230439443159961, 'y': -4.4400984293657455}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,810] Trial 41 finished with value: 4.8540586518935385 and parameters: {'x': 1.0126057987460935, 'y': -4.0490410415186755}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,815] Trial 42 finished with value: 9.432045745301798 and parameters: {'x': -0.07045248303071555, 'y': -5.066085510910581}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,820] Trial 43 finished with value: 17.640241451777335 and parameters: {'x': 1.0146597312905694, 'y': -1.2988291758933768}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,826] Trial 44 finished with value: 17.389955089600086 and parameters: {'x': 3.859069592656269, 'y': -9.080680644766689}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,831] Trial 45 finished with value: 170.57362945615873 and parameters: {'x': -9.844850426160052, 'y': -2.637068137729184}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,837] Trial 46 finished with value: 2.709647161187373 and parameters: {'x': 2.1713276990329007, 'y': -6.422304249729033}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,843] Trial 47 finished with value: 51.8736462148535 and parameters: {'x': 2.132273830949383, 'y': -12.149873950665022}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,848] Trial 48 finished with value: 2.522233876777757 and parameters: {'x': 3.382315213860623, 'y': -6.541450276210187}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,854] Trial 49 finished with value: 50.74026926783996 and parameters: {'x': -3.944076691590168, 'y': -6.58747225775889}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,859] Trial 50 finished with value: 18.706332087701778 and parameters: {'x': 3.0525473456704915, 'y': -9.324762521129312}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,864] Trial 51 finished with value: 8.210812757352715 and parameters: {'x': 5.547330407793178, 'y': -6.312219703740556}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,871] Trial 52 finished with value: 7.73827313303889 and parameters: {'x': 3.5225499962832254, 'y': -7.732254496642524}. Best is trial 24 with value: 0.9372281820877353.\n",
      "[I 2025-04-09 17:33:17,876] Trial 53 finished with value: 0.5778617856162962 and parameters: {'x': 2.45135884542544, 'y': -5.5261698101596}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:17,880] Trial 54 finished with value: 0.8821280090625186 and parameters: {'x': 2.1857759189506205, 'y': -5.468152918288258}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:17,887] Trial 55 finished with value: 28.44537397979198 and parameters: {'x': 2.4503755778982232, 'y': -10.305024691216929}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:17,891] Trial 56 finished with value: 3.31967472466901 and parameters: {'x': 4.635235648950668, 'y': -5.80354159635323}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:17,896] Trial 57 finished with value: 18.835954168995528 and parameters: {'x': -0.24382319867005675, 'y': -2.1166746238474072}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:17,903] Trial 58 finished with value: 3.1486999674245464 and parameters: {'x': 2.472151220435155, 'y': -3.3058702430638762}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:17,908] Trial 59 finished with value: 48.488410694385664 and parameters: {'x': 4.216108917741838, 'y': -11.856346679870734}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:17,913] Trial 60 finished with value: 9.607787776386093 and parameters: {'x': 1.7732728254392138, 'y': -7.8465642827064706}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:17,919] Trial 61 finished with value: 3.051136418383542 and parameters: {'x': 1.2536295429680038, 'y': -5.036423140850266}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:17,924] Trial 62 finished with value: 7.664776144053365 and parameters: {'x': 0.41849318925786894, 'y': -6.000299320276364}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:17,929] Trial 63 finished with value: 20.496625356862022 and parameters: {'x': 3.3576393117821555, 'y': -0.48682822401694903}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:17,935] Trial 64 finished with value: 14.279474351693313 and parameters: {'x': 6.585588896245765, 'y': -3.8070932061504603}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:17,941] Trial 65 finished with value: 5.8952031271743985 and parameters: {'x': 1.397728226220877, 'y': -6.824261025764981}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:17,946] Trial 66 finished with value: 15.214160964037317 and parameters: {'x': -0.9001421125040863, 'y': -4.944750870509306}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:17,954] Trial 67 finished with value: 19.421506038898638 and parameters: {'x': 0.4266396201983538, 'y': -8.577614064508044}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:17,963] Trial 68 finished with value: 8.329171502336283 and parameters: {'x': 2.398129479595471, 'y': -2.177426107432391}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:17,975] Trial 69 finished with value: 63.63609112318056 and parameters: {'x': -1.858153416490762, 'y': 1.3272771794050326}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:17,982] Trial 70 finished with value: 25.504115313122703 and parameters: {'x': 4.503628681091916, 'y': -9.82112187177637}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:17,992] Trial 71 finished with value: 2.136526129712798 and parameters: {'x': 3.402560596191887, 'y': -3.5948412559050738}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:17,998] Trial 72 finished with value: 0.6491224234543285 and parameters: {'x': 3.7114618573765004, 'y': -5.378079950477024}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:18,003] Trial 73 finished with value: 0.8193619661830047 and parameters: {'x': 3.6644352214915523, 'y': -5.614725794663341}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:18,009] Trial 74 finished with value: 3.8020377631036246 and parameters: {'x': 3.293943502176809, 'y': -3.07240175850058}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:18,014] Trial 75 finished with value: 7.620519190466971 and parameters: {'x': 5.694484861405851, 'y': -5.600225392766471}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:18,019] Trial 76 finished with value: 2.6436831898378657 and parameters: {'x': 3.8049534767356263, 'y': -3.5872958235606816}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:18,024] Trial 77 finished with value: 10.307490222155241 and parameters: {'x': 4.93555154024493, 'y': -7.561470370160608}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:18,029] Trial 78 finished with value: 10.628522491945336 and parameters: {'x': 2.8997017300595065, 'y': -1.7414017202189096}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:18,035] Trial 79 finished with value: 23.001847620517633 and parameters: {'x': 3.8009886776894635, 'y': -0.2713358378151951}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:18,040] Trial 80 finished with value: 374.71824109633667 and parameters: {'x': 4.394325878994236, 'y': 14.307358608559165}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:18,046] Trial 81 finished with value: 1.7825349920106492 and parameters: {'x': 3.771064682409738, 'y': -3.9100485113772177}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:18,051] Trial 82 finished with value: 1.176455406359091 and parameters: {'x': 2.9104864355018423, 'y': -3.9190547062270333}. Best is trial 53 with value: 0.5778617856162962.\n",
      "[I 2025-04-09 17:33:18,056] Trial 83 finished with value: 0.3578256997500343 and parameters: {'x': 2.747450984844714, 'y': -4.457741118381516}. Best is trial 83 with value: 0.3578256997500343.\n",
      "[I 2025-04-09 17:33:18,061] Trial 84 finished with value: 0.22759066808805012 and parameters: {'x': 2.580439375424079, 'y': -5.2270672816446995}. Best is trial 84 with value: 0.22759066808805012.\n",
      "[I 2025-04-09 17:33:18,067] Trial 85 finished with value: 0.06560033427246172 and parameters: {'x': 2.835738719764474, 'y': -5.196516070813172}. Best is trial 85 with value: 0.06560033427246172.\n",
      "[I 2025-04-09 17:33:18,072] Trial 86 finished with value: 0.02933233124383832 and parameters: {'x': 2.8559599303544054, 'y': -5.092654139574714}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,078] Trial 87 finished with value: 1.2560331154205597 and parameters: {'x': 1.9381000059667761, 'y': -5.358331575629052}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,084] Trial 88 finished with value: 0.17557442509054083 and parameters: {'x': 2.739018048978748, 'y': -4.672184737494299}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,089] Trial 89 finished with value: 5.452135949613817 and parameters: {'x': 2.6893253198159397, 'y': -7.314220644775763}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,095] Trial 90 finished with value: 2.133758921329188 and parameters: {'x': 1.5406655113533012, 'y': -4.935954925439452}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,101] Trial 91 finished with value: 4.860390724180441 and parameters: {'x': 5.06277003746636, 'y': -5.778055587160372}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,106] Trial 92 finished with value: 6.458605569943477 and parameters: {'x': 4.163511719146781, 'y': -2.7406093632681445}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,112] Trial 93 finished with value: 0.8306316997067804 and parameters: {'x': 2.2326772498153993, 'y': -4.5082200728416}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,118] Trial 94 finished with value: 5.237025915415172 and parameters: {'x': 0.7414198667577949, 'y': -4.63143291365235}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,125] Trial 95 finished with value: 12.660729525691906 and parameters: {'x': 2.5739859175485775, 'y': -8.532596994739844}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,130] Trial 96 finished with value: 4.802926056866722 and parameters: {'x': 2.3033304829421852, 'y': -7.07787815833103}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,136] Trial 97 finished with value: 224.230857994553 and parameters: {'x': 1.7885850019417704, 'y': 9.925258178572074}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,142] Trial 98 finished with value: 1.5961376085640362 and parameters: {'x': 2.9109366636676017, 'y': -6.260240187696527}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,147] Trial 99 finished with value: 5.511145523904448 and parameters: {'x': 0.7640734512875684, 'y': -5.7153866036398515}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,153] Trial 100 finished with value: 4.687488105882991 and parameters: {'x': 2.0850151581657785, 'y': -3.037784200171501}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,159] Trial 101 finished with value: 0.20573165862262088 and parameters: {'x': 3.1222133723465832, 'y': -4.563198500183097}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,165] Trial 102 finished with value: 0.09873753362747537 and parameters: {'x': 3.182025319925747, 'y': -5.256133395974451}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,171] Trial 103 finished with value: 0.4068699164977038 and parameters: {'x': 3.0562297383665653, 'y': -4.364619694182349}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,176] Trial 104 finished with value: 0.37084273205202806 and parameters: {'x': 3.112368227074147, 'y': -4.401488418160316}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,182] Trial 105 finished with value: 0.809280781948253 and parameters: {'x': 3.26019586139837, 'y': -4.138850247831765}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,188] Trial 106 finished with value: 18.002721033995808 and parameters: {'x': 5.499772181143236, 'y': -1.571609696026691}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,193] Trial 107 finished with value: 4.463110308548294 and parameters: {'x': 4.168314450159099, 'y': -6.760156712937157}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,199] Trial 108 finished with value: 3.110215799990055 and parameters: {'x': 4.763269543765937, 'y': -5.033110663199607}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,205] Trial 109 finished with value: 8.875326867955446 and parameters: {'x': 1.2533781707781184, 'y': -2.586571100355129}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,210] Trial 110 finished with value: 2.878992448001419 and parameters: {'x': 3.102107651555612, 'y': -3.306315709615509}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,216] Trial 111 finished with value: 0.7741722252687018 and parameters: {'x': 3.3008072293197124, 'y': -4.173146182171419}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,221] Trial 112 finished with value: 1.3895573003827266 and parameters: {'x': 4.108526326926768, 'y': -4.599092675431122}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,228] Trial 113 finished with value: 1.8160684322054705 and parameters: {'x': 2.595389592323716, 'y': -6.285441111138702}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,234] Trial 114 finished with value: 116.28072827622086 and parameters: {'x': -6.941235816607014, 'y': -0.8223740336369594}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,239] Trial 115 finished with value: 0.26465450568313664 and parameters: {'x': 3.4730229508036863, 'y': -5.202246863254069}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,245] Trial 116 finished with value: 8.820549408126059 and parameters: {'x': 2.822471548748194, 'y': -2.035369659279222}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,251] Trial 117 finished with value: 2.0031618535069624 and parameters: {'x': 1.5886044335977159, 'y': -5.10547231222903}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,257] Trial 118 finished with value: 21.888242469960467 and parameters: {'x': 6.461752084081644, 'y': -8.14714393987896}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,263] Trial 119 finished with value: 2.015339530134299 and parameters: {'x': 3.6329313911695866, 'y': -6.270723173711108}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,270] Trial 120 finished with value: 8.593770036780777 and parameters: {'x': 4.550737053727781, 'y': -7.4877669961184985}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,275] Trial 121 finished with value: 0.583029513077407 and parameters: {'x': 3.5155199315050014, 'y': -4.436733887670769}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,281] Trial 122 finished with value: 1.8298276486322864 and parameters: {'x': 3.207362958269778, 'y': -3.6632770473393177}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,288] Trial 123 finished with value: 0.19790592007567193 and parameters: {'x': 2.7743595554450504, 'y': -5.383395761396389}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,293] Trial 124 finished with value: 0.5928624047291702 and parameters: {'x': 2.4950636922929696, 'y': -4.418706846859211}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,299] Trial 125 finished with value: 4.494978686702094 and parameters: {'x': 1.9185962482997456, 'y': -6.823607581830781}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,305] Trial 126 finished with value: 4.258199540265809 and parameters: {'x': 2.921146781810931, 'y': -2.9379666078729385}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,311] Trial 127 finished with value: 45.43691618500703 and parameters: {'x': 9.729821048310933, 'y': -5.38265499175911}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,318] Trial 128 finished with value: 1.0312218537313311 and parameters: {'x': 3.977626513547851, 'y': -4.7252851410287295}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,324] Trial 129 finished with value: 1.4076228033272886 and parameters: {'x': 3.4365235882565686, 'y': -6.103208937700787}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,330] Trial 130 finished with value: 1.8846090792498904 and parameters: {'x': 2.1069001930786024, 'y': -3.9574157999821535}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,336] Trial 131 finished with value: 0.5052189940782807 and parameters: {'x': 2.51890734095777, 'y': -4.476770750536672}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,342] Trial 132 finished with value: 91.0141419114232 and parameters: {'x': 2.64640688226165, 'y': 4.533578227429156}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,347] Trial 133 finished with value: 5.263785496257991 and parameters: {'x': 1.3474937411306258, 'y': -3.408457175990688}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,354] Trial 134 finished with value: 0.6920866363530812 and parameters: {'x': 3.0177546366610635, 'y': -5.831727965881943}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,360] Trial 135 finished with value: 0.5491923554938857 and parameters: {'x': 2.383054159175522, 'y': -4.589427004074472}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,366] Trial 136 finished with value: 0.2330218256709515 and parameters: {'x': 2.5185331845779184, 'y': -4.965192941545274}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,372] Trial 137 finished with value: 1.3644156789718656 and parameters: {'x': 1.8345347097427944, 'y': -4.921856950548669}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,378] Trial 138 finished with value: 7.5394946723929275 and parameters: {'x': 2.2884969369542034, 'y': -2.3479709534642392}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,385] Trial 139 finished with value: 5.321555939620352 and parameters: {'x': 1.0758248103010541, 'y': -3.727559125551455}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,390] Trial 140 finished with value: 5.859847216702237 and parameters: {'x': 1.632635561596244, 'y': -6.997538913085554}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,396] Trial 141 finished with value: 0.14464478201041667 and parameters: {'x': 2.7088210017419785, 'y': -5.244662160915561}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,403] Trial 142 finished with value: 0.13275168818196637 and parameters: {'x': 2.738172498485532, 'y': -5.25337333646747}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,408] Trial 143 finished with value: 0.08806149763544678 and parameters: {'x': 2.9539671750906082, 'y': -5.2931594730966545}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,415] Trial 144 finished with value: 0.14246810319675154 and parameters: {'x': 3.026588624448094, 'y': -5.376511816875261}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,421] Trial 145 finished with value: 1.752883132988954 and parameters: {'x': 4.294822699174754, 'y': -5.276255879015741}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,427] Trial 146 finished with value: 2.078751354818473 and parameters: {'x': 3.9592186650950145, 'y': -6.0764064786834995}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,434] Trial 147 finished with value: 2.971199420782285 and parameters: {'x': 3.5355994288385375, 'y': -6.638393320485077}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,439] Trial 148 finished with value: 6.332052586120552 and parameters: {'x': 2.935500271794425, 'y': -7.515530236586704}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,444] Trial 149 finished with value: 1.2079616061181633 and parameters: {'x': 2.0059220432799423, 'y': -5.468796994531149}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,450] Trial 150 finished with value: 1.1796843171809817 and parameters: {'x': 2.6647442106086943, 'y': -6.033096255370522}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,456] Trial 151 finished with value: 0.03185254664048934 and parameters: {'x': 3.12882856608518, 'y': -5.123514157896679}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,462] Trial 152 finished with value: 0.06932165277676568 and parameters: {'x': 3.259696261752039, 'y': -5.043353251421111}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,469] Trial 153 finished with value: 0.9239902168703673 and parameters: {'x': 3.7712966856748324, 'y': -5.573665093532268}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,475] Trial 154 finished with value: 0.08137676967184734 and parameters: {'x': 3.272725049450228, 'y': -5.08365295615947}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,480] Trial 155 finished with value: 2.5510546394061 and parameters: {'x': 3.432077559784898, 'y': -6.537648731582226}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,486] Trial 156 finished with value: 2.7083793153840166 and parameters: {'x': 4.636447384572786, 'y': -5.174411217841359}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,492] Trial 157 finished with value: 1.2536797450015469 and parameters: {'x': 4.109113103202525, 'y': -5.153453150199047}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,499] Trial 158 finished with value: 30.382160264632642 and parameters: {'x': 8.25041874614084, 'y': -3.3221253757191818}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,505] Trial 159 finished with value: 4.525069099347404 and parameters: {'x': 3.1885217899387084, 'y': -7.118850781453406}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,510] Trial 160 finished with value: 1.5108632698152196 and parameters: {'x': 2.1903860743825545, 'y': -5.924872186446067}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,516] Trial 161 finished with value: 1.4972006199184873 and parameters: {'x': 2.768588101608005, 'y': -3.7984804815563336}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,522] Trial 162 finished with value: 98.64585353983568 and parameters: {'x': 2.753304019759785, 'y': -14.928997665080248}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,528] Trial 163 finished with value: 0.21257963523163356 and parameters: {'x': 3.4380674589283524, 'y': -5.1437933818355}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,535] Trial 164 finished with value: 0.4157974346075885 and parameters: {'x': 3.642871808403699, 'y': -4.9498674500215065}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,540] Trial 165 finished with value: 2.303257175753304 and parameters: {'x': 3.3533897405253823, 'y': -6.475931186419173}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,546] Trial 166 finished with value: 1.1411226214176753 and parameters: {'x': 3.8908717534146793, 'y': -5.589465978989059}. Best is trial 86 with value: 0.02933233124383832.\n",
      "[I 2025-04-09 17:33:18,554] Trial 167 finished with value: 0.005565909074504097 and parameters: {'x': 3.0745442382455446, 'y': -5.0030109166204975}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,559] Trial 168 finished with value: 0.7001198632226244 and parameters: {'x': 3.076571324091575, 'y': -4.166779323618594}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,565] Trial 169 finished with value: 1.4885424520835837 and parameters: {'x': 2.348796832651976, 'y': -6.031734891781549}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,572] Trial 170 finished with value: 10.681759495321705 and parameters: {'x': 1.6353217891178953, 'y': -7.969749631882345}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,577] Trial 171 finished with value: 0.0950659907952433 and parameters: {'x': 3.3027470121245273, 'y': -4.941602761667056}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,585] Trial 172 finished with value: 0.01651140677207989 and parameters: {'x': 3.1166429948273824, 'y': -4.946094355306896}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,591] Trial 173 finished with value: 3.2381744878227035 and parameters: {'x': 4.45274857132906, 'y': -3.938069646198924}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,596] Trial 174 finished with value: 0.6332489162422791 and parameters: {'x': 3.0914201432816553, 'y': -5.790500647466301}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,602] Trial 175 finished with value: 0.6852372220485659 and parameters: {'x': 3.8101901556409628, 'y': -4.8302085580748}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,609] Trial 176 finished with value: 3.369197409584487 and parameters: {'x': 3.2373039417681, 'y': -3.179866969475635}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,615] Trial 177 finished with value: 2.7934195115799905 and parameters: {'x': 2.790591090619462, 'y': -6.6581819623467275}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,621] Trial 178 finished with value: 1.6109834483303709 and parameters: {'x': 2.080605508925541, 'y': -4.124958733480353}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,627] Trial 179 finished with value: 4.724917499931605 and parameters: {'x': 5.0897262084332, 'y': -5.598299150692198}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,633] Trial 180 finished with value: 1.552824196801408 and parameters: {'x': 4.2273529270169785, 'y': -4.784526128395428}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,640] Trial 181 finished with value: 0.14431850496709153 and parameters: {'x': 2.6231125113329843, 'y': -5.047689892570246}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,646] Trial 182 finished with value: 0.36847407711880575 and parameters: {'x': 3.5068222876609774, 'y': -5.334073713196503}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,652] Trial 183 finished with value: 1.5796844703036328 and parameters: {'x': 2.8098563457394388, 'y': -6.24238877210319}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,658] Trial 184 finished with value: 0.4818493663148829 and parameters: {'x': 2.442327679494875, 'y': -4.586658798016321}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,664] Trial 185 finished with value: 1.6586332489214524 and parameters: {'x': 3.152381736127545, 'y': -3.7211673074963216}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,671] Trial 186 finished with value: 0.49573917973237214 and parameters: {'x': 3.656453786394293, 'y': -5.254573380503476}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,681] Trial 187 finished with value: 1.555580116036037 and parameters: {'x': 2.138313299136177, 'y': -5.901707350303}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,696] Trial 188 finished with value: 0.6631839221325377 and parameters: {'x': 2.850201846369325, 'y': -4.199534863156815}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,710] Trial 189 finished with value: 307.30659538027953 and parameters: {'x': 2.54056773013009, 'y': 12.52414098806791}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,717] Trial 190 finished with value: 4.767910547808164 and parameters: {'x': 3.2683796022276708, 'y': -2.8330013989583933}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,729] Trial 191 finished with value: 0.28345345297624325 and parameters: {'x': 2.4951852678721074, 'y': -4.830838718369459}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,738] Trial 192 finished with value: 1.5394106316005618 and parameters: {'x': 1.7598007034851246, 'y': -4.963718647702022}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,744] Trial 193 finished with value: 0.26169597811376294 and parameters: {'x': 2.942610528090049, 'y': -5.508332987939657}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,751] Trial 194 finished with value: 0.7296745693631216 and parameters: {'x': 3.812886341895539, 'y': -4.7375302597958955}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,757] Trial 195 finished with value: 1.439474818040517 and parameters: {'x': 3.4233844257155885, 'y': -6.122595406235924}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,765] Trial 196 finished with value: 4.826966646084741 and parameters: {'x': 2.4418396542348155, 'y': -7.124952628766117}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,772] Trial 197 finished with value: 1.434050518760621 and parameters: {'x': 2.0953172400209747, 'y': -4.215398430694053}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,777] Trial 198 finished with value: 0.07923790414634724 and parameters: {'x': 2.9540368814473923, 'y': -5.277714414244681}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,783] Trial 199 finished with value: 0.20667301720963738 and parameters: {'x': 3.0559953641554074, 'y': -5.451151345340719}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,790] Trial 200 finished with value: 2.582616017353602 and parameters: {'x': 3.171362065326932, 'y': -6.597889564369356}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,795] Trial 201 finished with value: 0.24621712487540612 and parameters: {'x': 2.805096128609813, 'y': -5.456321822612642}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,802] Trial 202 finished with value: 0.900331300273908 and parameters: {'x': 3.5179616684371164, 'y': -5.795013842837813}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,808] Trial 203 finished with value: 1.0574859288872278 and parameters: {'x': 4.020846129384589, 'y': -4.876068127554971}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,816] Trial 204 finished with value: 2.30730407986553 and parameters: {'x': 3.1491475302499783, 'y': -3.4883588077569923}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,823] Trial 205 finished with value: 0.14729635894029317 and parameters: {'x': 2.803308846210571, 'y': -5.329558718533248}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,828] Trial 206 finished with value: 1.6581135627608188 and parameters: {'x': 2.9471769818438096, 'y': -6.286593677706248}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,835] Trial 207 finished with value: 0.7077846088261177 and parameters: {'x': 3.5369155886548747, 'y': -4.352306971254509}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,841] Trial 208 finished with value: 1.1486567310904479 and parameters: {'x': 4.019893287689665, 'y': -5.329354539692433}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,847] Trial 209 finished with value: 0.7080391902162176 and parameters: {'x': 2.759019391515159, 'y': -5.8062056416017525}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,854] Trial 210 finished with value: 1.960931921622997 and parameters: {'x': 2.2142701475107667, 'y': -3.8408794193310842}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,859] Trial 211 finished with value: 0.08564974840790714 and parameters: {'x': 3.153892107203854, 'y': -5.2489316527649}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,865] Trial 212 finished with value: 0.10896087918027697 and parameters: {'x': 3.2070844229003224, 'y': -4.742945684781762}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,871] Trial 213 finished with value: 0.4214602912254404 and parameters: {'x': 3.100423064450438, 'y': -4.358614391062737}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,877] Trial 214 finished with value: 0.22403262958163783 and parameters: {'x': 2.626127640513561, 'y': -4.709737897076313}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,886] Trial 215 finished with value: 160.77077546627984 and parameters: {'x': -9.65309257327766, 'y': -5.818549814211447}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,893] Trial 216 finished with value: 2.2217884698327546 and parameters: {'x': 3.1284216759773464, 'y': -6.485024021008389}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,899] Trial 217 finished with value: 0.5347916650045315 and parameters: {'x': 3.7122858502860367, 'y': -5.165651841181525}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,906] Trial 218 finished with value: 0.8176625217485565 and parameters: {'x': 2.6314805459955517, 'y': -4.174254316530313}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,914] Trial 219 finished with value: 1.2159257125360974 and parameters: {'x': 2.049654500818744, 'y': -5.559257672921907}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,921] Trial 220 finished with value: 0.13555239229595847 and parameters: {'x': 3.3505951847859365, 'y': -4.887592666160639}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,928] Trial 221 finished with value: 0.1339298747260558 and parameters: {'x': 3.2977320328943818, 'y': -4.7871960730751555}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,934] Trial 222 finished with value: 0.37456142410597426 and parameters: {'x': 3.524478432283092, 'y': -4.684589476751577}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,941] Trial 223 finished with value: 1.956797075870544 and parameters: {'x': 3.2992332127837845, 'y': -3.63352403598247}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,947] Trial 224 finished with value: 1.7498925668076568 and parameters: {'x': 4.306818048748685, 'y': -4.794770483915357}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,955] Trial 225 finished with value: 1.1658887268277125 and parameters: {'x': 3.7974025549997847, 'y': -4.271962987130786}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,961] Trial 226 finished with value: 0.037161720150131784 and parameters: {'x': 2.8076430903076326, 'y': -5.012670416083704}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,967] Trial 227 finished with value: 1.2472214973870934 and parameters: {'x': 2.7765369430277516, 'y': -6.094205538075782}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,975] Trial 228 finished with value: 0.42259555841230895 and parameters: {'x': 2.357749613170398, 'y': -5.100548490936143}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,981] Trial 229 finished with value: 0.47616113269098986 and parameters: {'x': 2.82626435585235, 'y': -5.667815138076093}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,988] Trial 230 finished with value: 75.89287652283717 and parameters: {'x': 3.3676672794842384, 'y': -13.703889779543042}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:18,995] Trial 231 finished with value: 0.09557688263542273 and parameters: {'x': 3.002576092137211, 'y': -4.69085562210397}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,001] Trial 232 finished with value: 0.013511081289203555 and parameters: {'x': 2.8854615603047744, 'y': -5.019799674779822}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,007] Trial 233 finished with value: 0.03401416993719007 and parameters: {'x': 3.0669551850896464, 'y': -4.828153635107398}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,013] Trial 234 finished with value: 61.1490489296387 and parameters: {'x': 3.721193643453866, 'y': 2.786458030342194}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,020] Trial 235 finished with value: 0.1228767052010645 and parameters: {'x': 3.3493242076465104, 'y': -4.970857193800147}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,027] Trial 236 finished with value: 1.3819413233326352 and parameters: {'x': 3.2798187197040414, 'y': -3.8582282156946484}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,033] Trial 237 finished with value: 0.9693021050551309 and parameters: {'x': 3.9406994472422694, 'y': -4.709506187650718}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,039] Trial 238 finished with value: 0.954959492798903 and parameters: {'x': 3.4670277085536716, 'y': -4.1416034644513005}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,047] Trial 239 finished with value: 0.009961738913401486 and parameters: {'x': 3.098439057281082, 'y': -4.983523018631638}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,053] Trial 240 finished with value: 2.868001901592605 and parameters: {'x': 3.141326996969145, 'y': -3.3123896831554127}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,060] Trial 241 finished with value: 0.26392857327197605 and parameters: {'x': 2.486262605150733, 'y': -5.001569205360812}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,066] Trial 242 finished with value: 0.00721016399317673 and parameters: {'x': 3.0674833819218934, 'y': -5.051537919608387}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,072] Trial 243 finished with value: 0.13999174913510876 and parameters: {'x': 3.170090721714193, 'y': -4.666741998562895}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,078] Trial 244 finished with value: 0.7363743988347686 and parameters: {'x': 3.616198598859924, 'y': -4.402778361411917}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,085] Trial 245 finished with value: 0.8673617277882391 and parameters: {'x': 3.1346078204488865, 'y': -5.921543521740693}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,091] Trial 246 finished with value: 1.2358826504017801 and parameters: {'x': 4.067273551725055, 'y': -4.688857241463077}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,098] Trial 247 finished with value: 1.1742706337375417 and parameters: {'x': 3.4149948735373075, 'y': -3.998975580380131}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,105] Trial 248 finished with value: 0.42551278176914525 and parameters: {'x': 3.059717035471188, 'y': -5.649574212422013}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,111] Trial 249 finished with value: 45.3945152362748 and parameters: {'x': -3.628422393842846, 'y': -6.207696901162488}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,117] Trial 250 finished with value: 0.5866586616715675 and parameters: {'x': 3.7627982810107996, 'y': -5.069263584649777}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,127] Trial 251 finished with value: 0.23705632710497568 and parameters: {'x': 3.0553127533144306, 'y': -4.516267815391871}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,133] Trial 252 finished with value: 168.73171161823396 and parameters: {'x': 3.4383907992600515, 'y': 7.98227734742091}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,140] Trial 253 finished with value: 68.44565160135362 and parameters: {'x': -5.262148350135423, 'y': -5.427266008135602}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,146] Trial 254 finished with value: 2.949401169873606 and parameters: {'x': 4.159157840849371, 'y': -3.7328164024613364}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,154] Trial 255 finished with value: 3.0458037928262636 and parameters: {'x': 2.3068876600374404, 'y': -6.601686322916534}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,160] Trial 256 finished with value: 0.03583494162216241 and parameters: {'x': 2.9976078119237353, 'y': -4.810713922703306}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,166] Trial 257 finished with value: 0.8908601277947291 and parameters: {'x': 3.6598246277940603, 'y': -4.325098830678773}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,173] Trial 258 finished with value: 0.0065681599739911825 and parameters: {'x': 2.919874139588834, 'y': -5.012165790864613}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,179] Trial 259 finished with value: 1.2537264730727498 and parameters: {'x': 2.46213950286252, 'y': -5.982055272727443}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,185] Trial 260 finished with value: 0.042667493138196455 and parameters: {'x': 2.8306671095749727, 'y': -5.118295669229698}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,191] Trial 261 finished with value: 4.483708657105762 and parameters: {'x': 1.9569135510858247, 'y': -3.1572630906183656}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,197] Trial 262 finished with value: 0.10010110983239891 and parameters: {'x': 2.8975937384208383, 'y': -5.299356087998523}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,204] Trial 263 finished with value: 0.33691403637647194 and parameters: {'x': 2.7487848684980407, 'y': -5.523263790148836}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,209] Trial 264 finished with value: 32.84965126595186 and parameters: {'x': -2.5880042627391564, 'y': -6.274307508241585}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,215] Trial 265 finished with value: 4.310539980247092 and parameters: {'x': 2.308064702198757, 'y': -6.957489597393508}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,223] Trial 266 finished with value: 0.07965123998491101 and parameters: {'x': 2.9246690532156743, 'y': -5.271986191637532}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,234] Trial 267 finished with value: 1.140044936864901 and parameters: {'x': 2.9257843870123077, 'y': -3.934853540749554}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,249] Trial 268 finished with value: 1.1460087374412768 and parameters: {'x': 2.3998153067302943, 'y': -5.886446316144427}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,258] Trial 269 finished with value: 0.12390356246706986 and parameters: {'x': 2.75419131886635, 'y': -5.25195566027856}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,273] Trial 270 finished with value: 1.7340873708706663 and parameters: {'x': 1.888331072883746, 'y': -4.294110796686291}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,279] Trial 271 finished with value: 1.180242516065311 and parameters: {'x': 3.7693994772911728, 'y': -5.766985632466072}. Best is trial 167 with value: 0.005565909074504097.\n",
      "[I 2025-04-09 17:33:19,288] Trial 272 finished with value: 0.0031118768970528022 and parameters: {'x': 2.952955379964706, 'y': -5.029978002314825}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,294] Trial 273 finished with value: 2.060882741363588 and parameters: {'x': 2.8898028756974323, 'y': -6.431341795365116}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,299] Trial 274 finished with value: 2.6767271319002335 and parameters: {'x': 2.3881705582889765, 'y': -3.4826365411821034}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,306] Trial 275 finished with value: 0.03464503531955407 and parameters: {'x': 2.9720117871637, 'y': -5.184015475604048}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,312] Trial 276 finished with value: 0.4152579416460062 and parameters: {'x': 2.851427448223715, 'y': -5.627043968557779}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,318] Trial 277 finished with value: 31.317439963473745 and parameters: {'x': 2.299708933744347, 'y': 0.5522096849809506}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,324] Trial 278 finished with value: 1.0619573219613545 and parameters: {'x': 1.9844414541470061, 'y': -5.174923302925331}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,330] Trial 279 finished with value: 0.4589186736479918 and parameters: {'x': 2.9059757598073217, 'y': -4.329121385119349}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,337] Trial 280 finished with value: 1.5441720430493426 and parameters: {'x': 2.595399432829123, 'y': -6.174934221177657}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,360] Trial 281 finished with value: 2.753374621990917 and parameters: {'x': 4.619288186995536, 'y': -5.362326357649604}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,367] Trial 282 finished with value: 43.00080467923647 and parameters: {'x': 1.5523636496491202, 'y': -11.395713695622971}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,374] Trial 283 finished with value: 4.311939809773176 and parameters: {'x': 3.5690927284684366, 'y': -6.9970160931193135}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,380] Trial 284 finished with value: 0.8700481670084176 and parameters: {'x': 3.003489882484149, 'y': -4.067242803443112}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,387] Trial 285 finished with value: 1.1063399425423726 and parameters: {'x': 4.0238220376090705, 'y': -4.758901725745738}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,392] Trial 286 finished with value: 0.7777605058737231 and parameters: {'x': 2.5242388104959064, 'y': -5.742571071639189}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,399] Trial 287 finished with value: 0.14299236963796969 and parameters: {'x': 3.330490114500206, 'y': -5.183762493059956}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,405] Trial 288 finished with value: 1.7927261857627725 and parameters: {'x': 3.0561748004419993, 'y': -6.337748323698473}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,411] Trial 289 finished with value: 0.7488619653515943 and parameters: {'x': 3.6742194470416303, 'y': -4.457514882616608}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,418] Trial 290 finished with value: 2.4571857173526666 and parameters: {'x': 2.0112264972048113, 'y': -3.783647798734758}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,424] Trial 291 finished with value: 0.06652985261548389 and parameters: {'x': 2.7534033254866728, 'y': -5.075630236905963}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,434] Trial 292 finished with value: 0.25439467642666524 and parameters: {'x': 2.5664157580560962, 'y': -4.742319225465702}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,442] Trial 293 finished with value: 1.0066334171058247 and parameters: {'x': 3.4340473333419372, 'y': -5.904564165509876}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,448] Trial 294 finished with value: 6.089777779810314 and parameters: {'x': 4.283841444657435, 'y': -2.8925064828592655}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,456] Trial 295 finished with value: 1.229124729032828 and parameters: {'x': 2.2025372498485303, 'y': -4.22981957233796}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,462] Trial 296 finished with value: 0.011981538776530378 and parameters: {'x': 3.0666131882929824, 'y': -5.086857480518226}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,470] Trial 297 finished with value: 0.17040748716883986 and parameters: {'x': 2.6132943645763627, 'y': -4.85554849014136}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,477] Trial 298 finished with value: 1.5163342532776016 and parameters: {'x': 3.8606344789114804, 'y': -5.88070570963652}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,483] Trial 299 finished with value: 1.9388037904700748 and parameters: {'x': 3.136177526919997, 'y': -3.6142657283475925}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,491] Trial 300 finished with value: 5.224464670471999 and parameters: {'x': 1.6490718932370452, 'y': -6.843761893746006}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,497] Trial 301 finished with value: 0.2220023033320294 and parameters: {'x': 3.451734404723103, 'y': -5.133934054375626}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,504] Trial 302 finished with value: 0.4612921587282581 and parameters: {'x': 2.9257825107200275, 'y': -4.32488228951298}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,510] Trial 303 finished with value: 0.7434068485753951 and parameters: {'x': 2.338873755833189, 'y': -5.5534608729163075}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,517] Trial 304 finished with value: 91.91464444822272 and parameters: {'x': -6.586879566157576, 'y': -4.920096106396113}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,524] Trial 305 finished with value: 1.4242249644426532 and parameters: {'x': 2.691221075649677, 'y': -6.152770809970356}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,530] Trial 306 finished with value: 6.191717787930463 and parameters: {'x': 3.2335211123567875, 'y': -7.47733438962408}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,536] Trial 307 finished with value: 1.9961131304604076 and parameters: {'x': 3.775085868522362, 'y': -3.8187485335978018}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,542] Trial 308 finished with value: 0.9724157552346906 and parameters: {'x': 2.1688212999292706, 'y': -4.469379868659864}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,549] Trial 309 finished with value: 0.2287636250144438 and parameters: {'x': 2.9633780303391313, 'y': -5.476888305950777}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,556] Trial 310 finished with value: 2.6647721035178527 and parameters: {'x': 3.4991660802083846, 'y': -6.554221775644406}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,561] Trial 311 finished with value: 0.04191692917816976 and parameters: {'x': 2.8658363131477893, 'y': -4.845348668585131}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,569] Trial 312 finished with value: 3.0909298391996436 and parameters: {'x': 2.653631279393099, 'y': -3.276353125322712}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,575] Trial 313 finished with value: 1.5326877726857826 and parameters: {'x': 1.9759312177848556, 'y': -5.6956801721899035}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,582] Trial 314 finished with value: 1.010945871926972 and parameters: {'x': 4.002901642903868, 'y': -5.071653098939917}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,588] Trial 315 finished with value: 1.2203912400284396 and parameters: {'x': 2.5072135087344463, 'y': -4.011287345051848}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,595] Trial 316 finished with value: 0.2636007957900925 and parameters: {'x': 3.277518990150913, 'y': -4.568046291952818}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,602] Trial 317 finished with value: 1.2594794415117156 and parameters: {'x': 2.8496008197899707, 'y': -6.112141865098094}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,609] Trial 318 finished with value: 2.044706120532858 and parameters: {'x': 4.394068354360929, 'y': -5.318244471911563}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,615] Trial 319 finished with value: 0.3876100026224939 and parameters: {'x': 3.6202940330267004, 'y': -4.94665850382709}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,621] Trial 320 finished with value: 6.542242502669897 and parameters: {'x': 2.2565258812814855, 'y': -2.552656800228945}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,629] Trial 321 finished with value: 12.102479363118993 and parameters: {'x': -0.3634014487428048, 'y': -4.111174900380398}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,636] Trial 322 finished with value: 3.023071399529754 and parameters: {'x': 1.3822853411769886, 'y': -5.637236755185152}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,641] Trial 323 finished with value: 2.359335083603322 and parameters: {'x': 3.150869726101425, 'y': -6.528585427560201}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,648] Trial 324 finished with value: 0.3478845376590548 and parameters: {'x': 2.6118265323858108, 'y': -4.555921294476496}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,655] Trial 325 finished with value: 2.1931393720483503 and parameters: {'x': 3.417939483043972, 'y': -3.579272735335424}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,661] Trial 326 finished with value: 100.88366952444161 and parameters: {'x': 3.846155882711819, 'y': 5.008380975292346}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,668] Trial 327 finished with value: 0.03233135862260379 and parameters: {'x': 2.9521411752263, 'y': -5.173323084192164}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,674] Trial 328 finished with value: 1.7911608214214867 and parameters: {'x': 1.9174832340230374, 'y': -5.786967771131871}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,682] Trial 329 finished with value: 0.04847240690776227 and parameters: {'x': 2.850226356788968, 'y': -5.161369956023594}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,695] Trial 330 finished with value: 0.4267934881300081 and parameters: {'x': 2.3712507171195716, 'y': -5.177391734326476}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,716] Trial 331 finished with value: 1.3962599965790639 and parameters: {'x': 2.787376066952534, 'y': -6.16234721992806}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,726] Trial 332 finished with value: 0.34809970497901194 and parameters: {'x': 2.9683156265062514, 'y': -4.410851626960304}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,741] Trial 333 finished with value: 0.6683021097600824 and parameters: {'x': 2.4284903995376363, 'y': -5.584533049826468}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,749] Trial 334 finished with value: 1.665695413123759 and parameters: {'x': 1.7195824710419312, 'y': -4.838054439206652}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,759] Trial 335 finished with value: 4.038859340029499 and parameters: {'x': 3.2452296483987673, 'y': -6.9946733465847775}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,769] Trial 336 finished with value: 1.3183647869528365 and parameters: {'x': 2.6623606922140386, 'y': -3.902564587417258}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,777] Trial 337 finished with value: 29.663439193652188 and parameters: {'x': 8.335807514637105, 'y': -6.092061060744491}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,792] Trial 338 finished with value: 0.2902371287448482 and parameters: {'x': 3.5066673401291792, 'y': -5.183099249565013}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,803] Trial 339 finished with value: 1.04463113413567 and parameters: {'x': 2.2024808748922915, 'y': -4.3607861240374195}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,814] Trial 340 finished with value: 4.1480211792652355 and parameters: {'x': 3.0104524781514512, 'y': -2.963357683596423}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,823] Trial 341 finished with value: 1.2855257073669355 and parameters: {'x': 3.9926970038791345, 'y': -5.5477940907460805}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,832] Trial 342 finished with value: 0.13079397673848503 and parameters: {'x': 2.6890061823914775, 'y': -4.815400915095004}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,841] Trial 343 finished with value: 2.553570317029268 and parameters: {'x': 3.5661454172278724, 'y': -6.494339213023986}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,852] Trial 344 finished with value: 2.169913091081731 and parameters: {'x': 3.1720217798560237, 'y': -3.537016200247967}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,868] Trial 345 finished with value: 46.25365877815365 and parameters: {'x': 2.1836279076890426, 'y': 1.7518290399749814}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,875] Trial 346 finished with value: 20.57799077184714 and parameters: {'x': 2.8612515792884614, 'y': -9.534174637968546}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,885] Trial 347 finished with value: 1.2313419316401104 and parameters: {'x': 3.7245483626045988, 'y': -4.159540838655973}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,897] Trial 348 finished with value: 8.4237658207594 and parameters: {'x': 0.11309346053661187, 'y': -5.299226424573305}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,905] Trial 349 finished with value: 19.462246502542722 and parameters: {'x': -1.2991979726471614, 'y': -5.989516697448435}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,913] Trial 350 finished with value: 0.2417992764528626 and parameters: {'x': 2.569577952715952, 'y': -4.7622267095221344}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,927] Trial 351 finished with value: 0.4396453823202614 and parameters: {'x': 3.194309529157341, 'y': -5.633947307904146}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,935] Trial 352 finished with value: 2.2494950958753375 and parameters: {'x': 4.314818136172625, 'y': -4.278371033933031}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,943] Trial 353 finished with value: 6.7884188170581785 and parameters: {'x': 2.0150986965209885, 'y': -7.412133545113853}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,952] Trial 354 finished with value: 3.198310652241604 and parameters: {'x': 4.787604211067036, 'y': -5.052743121039661}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,959] Trial 355 finished with value: 3.3694747994371226 and parameters: {'x': 3.359573830343507, 'y': -6.80005040484127}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,968] Trial 356 finished with value: 1.6594915167490452 and parameters: {'x': 2.9016031193691054, 'y': -3.7155508688036125}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,975] Trial 357 finished with value: 0.3459110646057179 and parameters: {'x': 2.510556198903692, 'y': -4.673877584066787}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,983] Trial 358 finished with value: 1.5945455652495228 and parameters: {'x': 3.7955796883718143, 'y': -5.980611301535797}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:19,992] Trial 359 finished with value: 0.11991970373249404 and parameters: {'x': 3.102903195805948, 'y': -5.330651835055269}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,000] Trial 360 finished with value: 1.8479586374903239 and parameters: {'x': 1.7631836314232225, 'y': -4.435868891204468}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,008] Trial 361 finished with value: 2.183948919827993 and parameters: {'x': 2.3983986086425064, 'y': -6.349823946203623}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,016] Trial 362 finished with value: 0.24821295803488874 and parameters: {'x': 3.481832314011746, 'y': -5.126690880528058}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,025] Trial 363 finished with value: 3.336586687122799 and parameters: {'x': 2.796872895882199, 'y': -3.184696701183101}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,033] Trial 364 finished with value: 1.4158801096528553 and parameters: {'x': 4.044436175515268, 'y': -5.570116816913775}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,042] Trial 365 finished with value: 0.7690271603136108 and parameters: {'x': 3.1233374819211686, 'y': -4.131774783902842}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,054] Trial 366 finished with value: 63.56851908843953 and parameters: {'x': 2.589500252475984, 'y': -12.962412262984268}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,062] Trial 367 finished with value: 0.22274561580383034 and parameters: {'x': 3.443794793974562, 'y': -4.839401753917092}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,074] Trial 368 finished with value: 4.615035828330121 and parameters: {'x': 1.0379387821051722, 'y': -5.874843760658545}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,080] Trial 369 finished with value: 2.178726256785386 and parameters: {'x': 2.1504384158077214, 'y': -3.792949308665915}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,093] Trial 370 finished with value: 0.003954102654455756 and parameters: {'x': 2.938193040891702, 'y': -5.011575943168526}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,102] Trial 371 finished with value: 0.37000187553659325 and parameters: {'x': 2.668152918002061, 'y': -4.490216330483168}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,110] Trial 372 finished with value: 2.343304051104128 and parameters: {'x': 1.4703294699394762, 'y': -4.941586640496509}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,118] Trial 373 finished with value: 0.36003094832535926 and parameters: {'x': 3.013331890468529, 'y': -5.599877661712698}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,127] Trial 374 finished with value: 2.697351853130056 and parameters: {'x': 2.2357690514876176, 'y': -6.453720368731869}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,137] Trial 375 finished with value: 1.6446792131061487 and parameters: {'x': 3.7309027795153242, 'y': -3.9462161796635318}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,144] Trial 376 finished with value: 0.1801652716718737 and parameters: {'x': 2.8214792217006286, 'y': -4.614908318205573}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,153] Trial 377 finished with value: 0.15970271954651105 and parameters: {'x': 3.3089408685238655, 'y': -5.2534921286790395}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,161] Trial 378 finished with value: 1.9802339654639147 and parameters: {'x': 1.890357320449166, 'y': -5.86540573674038}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,178] Trial 379 finished with value: 9.189039908280424 and parameters: {'x': 2.434062936985307, 'y': -2.0219544749976155}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,195] Trial 380 finished with value: 2.7844932301727177 and parameters: {'x': 3.6686295018374304, 'y': -3.4711351206057}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,204] Trial 381 finished with value: 0.35431141241739744 and parameters: {'x': 2.8927156631017317, 'y': -4.414507486406789}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,214] Trial 382 finished with value: 29.20435631662374 and parameters: {'x': 4.1760082237408955, 'y': -10.274595811464375}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,224] Trial 383 finished with value: 0.1569007479449646 and parameters: {'x': 3.3908772198869706, 'y': -5.064154087308585}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,233] Trial 384 finished with value: 55.97195624593662 and parameters: {'x': -4.282297042550396, 'y': -6.714673738644869}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,241] Trial 385 finished with value: 1.1679034772832264 and parameters: {'x': 2.54015858446325, 'y': -5.977982285034031}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,249] Trial 386 finished with value: 144.78287424306743 and parameters: {'x': -9.001909234258177, 'y': -4.141484434797054}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,258] Trial 387 finished with value: 0.07890034336461849 and parameters: {'x': 3.0027668015999396, 'y': -5.28087842240643}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,265] Trial 388 finished with value: 1.0016296516685492 and parameters: {'x': 2.195500461783815, 'y': -5.595323563013001}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,275] Trial 389 finished with value: 8.247159332691936 and parameters: {'x': 2.9509790485772815, 'y': -7.871368363518263}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,285] Trial 390 finished with value: 0.10003138551063342 and parameters: {'x': 2.812733527404231, 'y': -4.745122276861685}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,297] Trial 391 finished with value: 5.909578355366153 and parameters: {'x': 1.8234548423440018, 'y': -7.127279917491437}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,306] Trial 392 finished with value: 1.6913843093255236 and parameters: {'x': 3.146189310844098, 'y': -6.292289826130521}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,314] Trial 393 finished with value: 233.61871479207943 and parameters: {'x': 2.4897094234619908, 'y': 10.276070120275566}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,322] Trial 394 finished with value: 0.3883855478187272 and parameters: {'x': 3.4824044861834094, 'y': -5.3945522266175265}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,330] Trial 395 finished with value: 5.53432795901078 and parameters: {'x': 3.891474072407876, 'y': -2.8229373143532825}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,339] Trial 396 finished with value: 0.1370446060537947 and parameters: {'x': 2.735339084554157, 'y': -4.741157951852557}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,348] Trial 397 finished with value: 1.7119740524808615 and parameters: {'x': 2.2721874440181047, 'y': -3.912680849135838}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,356] Trial 398 finished with value: 0.4407171656280354 and parameters: {'x': 3.266133690196386, 'y': -5.608185847065261}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,363] Trial 399 finished with value: 0.04290976865300161 and parameters: {'x': 2.8028371400094523, 'y': -5.063534048299705}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,373] Trial 400 finished with value: 0.5718216604437016 and parameters: {'x': 2.432322712309099, 'y': -4.50043603264086}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,381] Trial 401 finished with value: 5.0790531829297345 and parameters: {'x': 1.4563890822314225, 'y': -3.357952949067724}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,390] Trial 402 finished with value: 2.1429432993926825 and parameters: {'x': 3.610351735156529, 'y': -6.330569073285599}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,398] Trial 403 finished with value: 1.0406435400455163 and parameters: {'x': 1.9800050124783342, 'y': -5.015930018085142}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,407] Trial 404 finished with value: 1.1780053205676757 and parameters: {'x': 3.104349556013373, 'y': -3.9196683422543392}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,414] Trial 405 finished with value: 1.025098788354497 and parameters: {'x': 2.623244314905842, 'y': -5.939762705210059}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,423] Trial 406 finished with value: 0.21929063552817102 and parameters: {'x': 3.0270123211034927, 'y': -4.532494951859584}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,432] Trial 407 finished with value: 2.2560270608559456 and parameters: {'x': 4.470770095880851, 'y': -5.304733302936484}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,440] Trial 408 finished with value: 1.314210761714518 and parameters: {'x': 3.857304335469852, 'y': -4.238921792389827}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,448] Trial 409 finished with value: 0.21633563206908343 and parameters: {'x': 3.464919868658226, 'y': -4.986393097490559}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,456] Trial 410 finished with value: 0.6308258125450886 and parameters: {'x': 2.702527309681864, 'y': -5.7364345259831175}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,463] Trial 411 finished with value: 9.158601396589264 and parameters: {'x': 5.578669599446396, 'y': -6.58400268102053}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,472] Trial 412 finished with value: 2.461954240905645 and parameters: {'x': 2.1225389141332967, 'y': -3.6992247374372194}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,478] Trial 413 finished with value: 0.008823468280201417 and parameters: {'x': 3.076331738956774, 'y': -4.945255740866861}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,487] Trial 414 finished with value: 14.295676990659326 and parameters: {'x': 6.7347606062958745, 'y': -4.4107290908929935}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,494] Trial 415 finished with value: 0.15663527755400614 and parameters: {'x': 3.38186643883888, 'y': -4.896012979451673}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,503] Trial 416 finished with value: 4.420360752056465 and parameters: {'x': 4.098514252903897, 'y': -3.2073407495501387}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,511] Trial 417 finished with value: 14.091913894357502 and parameters: {'x': 2.437792694863105, 'y': -8.711581447362864}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,520] Trial 418 finished with value: 14.889535839249353 and parameters: {'x': 3.1149584934717742, 'y': -1.1430166730943658}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,528] Trial 419 finished with value: 0.5088532304739726 and parameters: {'x': 2.7509365798993817, 'y': -5.6684464400696255}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,539] Trial 420 finished with value: 1.2130775881151554 and parameters: {'x': 3.7374570627175485, 'y': -4.181932356853566}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,547] Trial 421 finished with value: 0.009614532630734763 and parameters: {'x': 3.0933630655626985, 'y': -4.970035510692321}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,558] Trial 422 finished with value: 23.54389387206253 and parameters: {'x': 3.404069362949377, 'y': -0.164648738510488}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,565] Trial 423 finished with value: 0.6242671483755041 and parameters: {'x': 2.266554593246001, 'y': -4.70618886391601}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,576] Trial 424 finished with value: 3.4549535336066945 and parameters: {'x': 1.7148162144819095, 'y': -3.657146258504641}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,587] Trial 425 finished with value: 0.9304422410968864 and parameters: {'x': 3.135077134548742, 'y': -5.955089738620925}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,595] Trial 426 finished with value: 0.1301252679450742 and parameters: {'x': 2.6521092491270477, 'y': -4.904620267340867}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,604] Trial 427 finished with value: 6.485795164946568 and parameters: {'x': 3.6496689537505222, 'y': -2.5375367179429316}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,613] Trial 428 finished with value: 2.2851698852743265 and parameters: {'x': 4.2174954195820575, 'y': -4.103967194478329}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,622] Trial 429 finished with value: 0.1925750815423356 and parameters: {'x': 2.731707068482941, 'y': -5.3472664458889145}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,632] Trial 430 finished with value: 4.31813191252846 and parameters: {'x': 3.2850625073032136, 'y': -7.058366167487812}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,641] Trial 431 finished with value: 2.148987191839015 and parameters: {'x': 2.147351088474075, 'y': -6.192466781722942}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,651] Trial 432 finished with value: 0.2955722900272558 and parameters: {'x': 2.9313612319266005, 'y': -4.4606846844891}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,672] Trial 433 finished with value: 0.7017802671145393 and parameters: {'x': 3.8255754107219233, 'y': -5.142146080937418}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,682] Trial 434 finished with value: 3.555890619413578 and parameters: {'x': 2.439466785028566, 'y': -3.1995297463364283}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,692] Trial 435 finished with value: 0.7469143282083007 and parameters: {'x': 3.3896762320501685, 'y': -5.771405705438768}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,703] Trial 436 finished with value: 0.1476572731441856 and parameters: {'x': 3.029736079939553, 'y': -4.616890304620186}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,711] Trial 437 finished with value: 2.632568570142686 and parameters: {'x': 1.9045635159019407, 'y': -3.803092535134997}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,722] Trial 438 finished with value: 6.83591082648555 and parameters: {'x': 0.8063969281975623, 'y': -6.422679299724453}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,732] Trial 439 finished with value: 0.31601304199405644 and parameters: {'x': 2.5944096360060662, 'y': -5.389242210749714}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,743] Trial 440 finished with value: 0.40638703843559604 and parameters: {'x': 3.630749097381834, 'y': -4.907573734319741}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,753] Trial 441 finished with value: 3.8187351511397525 and parameters: {'x': 4.73018327412599, 'y': -4.091594259664536}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,761] Trial 442 finished with value: 0.6848616211458795 and parameters: {'x': 3.178123105199122, 'y': -5.808166926160742}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,773] Trial 443 finished with value: 0.3188570374340013 and parameters: {'x': 2.7792998987879995, 'y': -4.48024188052613}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,780] Trial 444 finished with value: 1.0116431960250343 and parameters: {'x': 3.9902637275608863, 'y': -5.176127640937625}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,789] Trial 445 finished with value: 3.4174858717662677 and parameters: {'x': 2.406878081532688, 'y': -6.750911837186534}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,796] Trial 446 finished with value: 1.0983545669669719 and parameters: {'x': 3.423977193856217, 'y': -5.958435133984966}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,805] Trial 447 finished with value: 0.0719636361305776 and parameters: {'x': 2.876594314871874, 'y': -4.761809586656735}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,814] Trial 448 finished with value: 3.1389927434810683 and parameters: {'x': 2.1380424074074806, 'y': -3.4520911357404866}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,824] Trial 449 finished with value: 0.28610653782224654 and parameters: {'x': 3.2653018246016074, 'y': -5.4644582647400135}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,832] Trial 450 finished with value: 3.7234547723388394 and parameters: {'x': 1.2071445959272475, 'y': -4.286470552516599}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,841] Trial 451 finished with value: 419.3631174169836 and parameters: {'x': 7.619947999659617, 'y': 14.950418489280485}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,848] Trial 452 finished with value: 0.22133024692956052 and parameters: {'x': 2.537277664716865, 'y': -4.915039495295269}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,859] Trial 453 finished with value: 1.8150948605754451 and parameters: {'x': 3.6805873624718632, 'y': -6.162710498197654}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,867] Trial 454 finished with value: 0.979353611771032 and parameters: {'x': 3.1395490830839763, 'y': -4.020265512916151}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,876] Trial 455 finished with value: 1.6828475364837026 and parameters: {'x': 1.81214023689701, 'y': -5.521379631060335}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,886] Trial 456 finished with value: 0.10838216411533358 and parameters: {'x': 2.8151052591460486, 'y': -4.727610391314419}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,895] Trial 457 finished with value: 1.3933184822714055 and parameters: {'x': 4.143107976621029, 'y': -5.294317237104425}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,906] Trial 458 finished with value: 5.824677430557765 and parameters: {'x': 2.3627608525912454, 'y': -7.327789444852676}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,913] Trial 459 finished with value: 1.978542370126698 and parameters: {'x': 3.5288227728603756, 'y': -3.6965856970897697}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,923] Trial 460 finished with value: 28.702705929392053 and parameters: {'x': -2.185833357462606, 'y': -6.345302314730845}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,931] Trial 461 finished with value: 0.3576847491859206 and parameters: {'x': 2.921988621871196, 'y': -4.407042181881067}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,940] Trial 462 finished with value: 0.48452585995235264 and parameters: {'x': 3.1383364430030225, 'y': -5.682194172131091}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,951] Trial 463 finished with value: 4.356034184924278 and parameters: {'x': 2.5081795969946894, 'y': -2.971664013011658}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,960] Trial 464 finished with value: 0.6989946645563959 and parameters: {'x': 3.7972677604389036, 'y': -5.251711705570343}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,970] Trial 465 finished with value: 0.21631988647687908 and parameters: {'x': 3.3504630915800364, 'y': -4.694229648400892}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,978] Trial 466 finished with value: 1.406772313533342 and parameters: {'x': 2.131166097227837, 'y': -4.1925967779807705}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,990] Trial 467 finished with value: 1.414005685299459 and parameters: {'x': 2.777072781349733, 'y': -6.168036446556491}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:20,997] Trial 468 finished with value: 1.9176012408271943 and parameters: {'x': 4.384766507481649, 'y': -5.0047917203874475}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,009] Trial 469 finished with value: 3.410778293439038 and parameters: {'x': 2.8993112160777033, 'y': -6.844082444532046}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,016] Trial 470 finished with value: 2.1650158027968804 and parameters: {'x': 1.6973079849705623, 'y': -5.684112064486093}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,026] Trial 471 finished with value: 1.6543370517549365 and parameters: {'x': 3.5336415767524807, 'y': -3.829716393639532}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,036] Trial 472 finished with value: 0.5863092351022127 and parameters: {'x': 2.2982140350478684, 'y': -4.693722846920607}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,045] Trial 473 finished with value: 76.24952268652859 and parameters: {'x': 3.161419228414, 'y': 3.730605163402296}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,056] Trial 474 finished with value: 0.14671213466624397 and parameters: {'x': 2.7502669510393782, 'y': -5.290423034422333}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,066] Trial 475 finished with value: 41.87729934987683 and parameters: {'x': 4.013232227647391, 'y': 1.3914520887458373}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,078] Trial 476 finished with value: 1.0688310730565376 and parameters: {'x': 3.286291907888526, 'y': -5.993412309433543}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,087] Trial 477 finished with value: 0.5882989168252832 and parameters: {'x': 2.5442722838287173, 'y': -4.3830630781525635}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,095] Trial 478 finished with value: 2.435541753606192 and parameters: {'x': 3.6135342439736853, 'y': -3.565037462134349}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,112] Trial 479 finished with value: 0.8975534047217805 and parameters: {'x': 2.052841733204871, 'y': -5.02108607036476}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,132] Trial 480 finished with value: 0.17641171745689224 and parameters: {'x': 2.998788850260455, 'y': -5.42001220288606}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,152] Trial 481 finished with value: 2.8148862913387407 and parameters: {'x': 2.533212709560785, 'y': -6.6115197537800015}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,171] Trial 482 finished with value: 0.46178756209686195 and parameters: {'x': 3.2568642012751767, 'y': -4.37086699005685}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,180] Trial 483 finished with value: 6.936134580458202 and parameters: {'x': 3.9473426185573945, 'y': -2.5426281226637633}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,197] Trial 484 finished with value: 0.42911876271649996 and parameters: {'x': 2.856331252877308, 'y': -5.639122878495752}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,207] Trial 485 finished with value: 0.664853835540296 and parameters: {'x': 2.2003695986180447, 'y': -4.8404849326050075}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,218] Trial 486 finished with value: 3.0497038465057873 and parameters: {'x': 1.5445548435849366, 'y': -4.034918012201182}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,227] Trial 487 finished with value: 1.5140832839431846 and parameters: {'x': 3.558282938730751, 'y': -6.09654158346382}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,241] Trial 488 finished with value: 0.014030049331902963 and parameters: {'x': 3.1065151460066334, 'y': -4.948187134772447}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,248] Trial 489 finished with value: 3.4881533867152803 and parameters: {'x': 2.6200169182793758, 'y': -3.1714032034586253}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,256] Trial 490 finished with value: 0.0096528087060963 and parameters: {'x': 2.983806466532232, 'y': -4.903095004360329}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,265] Trial 491 finished with value: 0.9749915102531355 and parameters: {'x': 3.045475559552027, 'y': -4.013631162426263}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,274] Trial 492 finished with value: 0.42344940327738845 and parameters: {'x': 3.5532082874695377, 'y': -4.65734858244506}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,282] Trial 493 finished with value: 0.3436411293337781 and parameters: {'x': 3.1982536918429543, 'y': -5.551667112491235}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,289] Trial 494 finished with value: 2.1019303600643475 and parameters: {'x': 4.448120978360736, 'y': -4.930171695596323}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,299] Trial 495 finished with value: 2.9076168156555 and parameters: {'x': 2.2770690692588538, 'y': -3.4556594659748}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,306] Trial 496 finished with value: 1.3315391190031662 and parameters: {'x': 3.8586281939569385, 'y': -4.229093557203332}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,314] Trial 497 finished with value: 7.071626640113207 and parameters: {'x': 2.8451979459688532, 'y': -7.654743483687444}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,322] Trial 498 finished with value: 2.2903993908538482 and parameters: {'x': 1.928754878848051, 'y': -6.069033807352225}. Best is trial 272 with value: 0.0031118768970528022.\n",
      "[I 2025-04-09 17:33:21,329] Trial 499 finished with value: 0.13726153259435514 and parameters: {'x': 3.3130894694412025, 'y': -5.19808209590819}. Best is trial 272 with value: 0.0031118768970528022.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0031118768970528022\n",
      "{'x': 2.952955379964706, 'y': -5.029978002314825}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "# 목적 함수\n",
    "def objective(trial): # 이 함수는 Optuna가 여러 번 호출하면서, x와 y라는 값을 바꿔보고, 그 결과(loss)를 리턴받는 곳.\n",
    "    x = trial.suggest_uniform('x', -10, 10) # trial.suggest_uniform(...)은 말 그대로 x와 y를 지정된 범위 내에서 랜덤하게 선택하는 함수\n",
    "    y = trial.suggest_uniform('y', -15, 15)\n",
    "    return (x -3) ** 2 + (y + 5) ** 2 # 2차원 함수의 최솟값 문제 -> 최솟값을 자동으로 찾아줄거임 \n",
    "    # 수학적으로 보면, (3, -5)에서 가장 작은 값이 되는 2차 함수, 대입 해보면 0 \n",
    "\n",
    "# 스터디 생성\n",
    "study = optuna.create_study(direction=\"minimize\") # Optuna의 스터디 객체를 생성, direction=\"minimize\"는 → 목적 함수의 결과를 최소화하고 싶다는 뜻 (loss 최소화)\n",
    "\n",
    "# 최적화 실행\n",
    "study.optimize(objective, n_trials=500) # objective() 함수를 500번 실행하면서, 매번 x, y를 바꿔보고 → 결과를 비교 → 점점 더 좋은 조합 탐색. \n",
    "    \n",
    "# 결과 확인\n",
    "print(study.best_value) # \t최적의 x, y 조합일 때의 수식 결과값 (최소값)\n",
    "print(study.best_params) # 그때 사용된 x, y 값 (거의 3과 -5에 가까울 것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "cliponaxis": false,
         "hovertemplate": [
          "x (FloatDistribution): 0.0792968831139647<extra></extra>",
          "y (FloatDistribution): 0.9207031168860353<extra></extra>"
         ],
         "name": "Objective Value",
         "orientation": "h",
         "text": [
          "0.08",
          "0.92"
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          0.0792968831139647,
          0.9207031168860353
         ],
         "y": [
          "x",
          "y"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Hyperparameter Importances"
        },
        "xaxis": {
         "title": {
          "text": "Hyperparameter Importance"
         }
        },
        "yaxis": {
         "title": {
          "text": "Hyperparameter"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import optuna.visualization as vis\n",
    "\n",
    "# 하이퍼 파라미터 중요도 시각화\n",
    "vis.plot_param_importances(study).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "markers",
         "name": "Objective Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          7.935048465077573,
          452.27890850680484,
          250.42669018743453,
          66.42659304000269,
          204.91343527004756,
          17.76596687515749,
          21.40108917906872,
          330.4673971733978,
          315.4956680638626,
          101.23390808994931,
          58.63270533895822,
          17.750362570971166,
          5.598382393610107,
          50.47285686356848,
          41.923488502006016,
          31.678930215708483,
          4.849724113540047,
          37.31546855951744,
          55.88465244064079,
          76.17210129107352,
          59.18696739989606,
          8.844303635863726,
          3.900574027615415,
          6.005693649306107,
          31.33502503599535,
          45.52467438045163,
          15.617037328592327,
          45.11876008856599,
          8.384363619934874,
          52.63383426293184,
          30.366460439379935,
          2.6035303780248436,
          3.491095362149336,
          390.3157708642751,
          2.2976683955039836,
          15.734558066823606,
          24.681601582831945,
          94.094507550802,
          8.545165951976117,
          9.181212882342395,
          42.32123321269057,
          13.851346857390388,
          4.37904003573593,
          88.74188993282445,
          19.24822144370416,
          1.3208993944055742,
          4.975891282506667,
          264.550646959259,
          10.843168974994171,
          8.978822128370247,
          136.498758012209,
          3.8101202065755095,
          13.55139138774173,
          3.6130829838705707,
          9.389296660501932,
          17.07972122029765,
          0.13144875827851313,
          23.226561258791122,
          52.99557108439154,
          88.56873914268604,
          4.486607871858744,
          13.706395816433597,
          3.6995406830546465,
          10.915195757447208,
          2.0080273320495583,
          3.8439948845298852,
          0.04331950282309835,
          0.10239669278321122,
          29.08895099855308,
          0.260690572670981,
          8.10506546949457,
          8.443536440838002,
          3.048075480423596,
          0.017934197711261163,
          0.19344072831119646,
          0.0923428299286315,
          96.40773123851018,
          0.03471703936099302,
          5.210245963865794,
          11.299774523038455,
          0.7220694037174592,
          0.22329023454632993,
          0.4075151027698742,
          12.52219353859632,
          14.371348851376062,
          6.449350486605155,
          30.554695514755544,
          0.011397773776140332,
          20.19180374623467,
          60.71216051827795,
          9.497700231691555,
          0.7662726532454656,
          0.31187186536653605,
          4.349053708256902,
          1.584660864451018,
          9.44768480465933,
          22.53843457942267,
          3.142152463694741,
          7.476594547854452,
          16.161247553753725,
          8.244392402196803,
          0.2979221815080293,
          2.0815726723615353,
          0.5285406936494912,
          1.2652561430436735,
          3.5889434355258247,
          3.3735071369197396,
          2.207574513801756,
          4.9822523883007825,
          1.6394747757256438,
          1.0091189072210986,
          0.13077745338130498,
          1.0004973247148785,
          4.7082829154529335,
          0.3861773789822325,
          8.155913510593656,
          4.382160851199977,
          0.37980941459741885,
          7.548905878240566,
          3.3973141899350363,
          0.36241482221060883,
          0.5048636345685832,
          1.7782854581701912,
          1.9377758548777617,
          1.5402208758128646,
          5.319753858972254,
          5.18423387119135,
          185.80160714225295,
          4.796719731743476,
          13.806751905227767,
          1.1581552716448018,
          0.2485285983913307,
          2.2114150693400383,
          0.007708817239161022,
          1.1039528421599507,
          1.0088825486404716,
          4.963384621472613,
          2.163406349536845,
          1.399015078508529,
          11.396515794620424,
          4.965527553891133,
          0.2813099540564796,
          0.531911709524178,
          1.4416258645391684,
          2.23012801536555,
          88.4051387098028,
          1.0333233983221561,
          3.43350173423553,
          2.9186667795230847,
          0.810803942516491,
          6.542166564648654,
          0.2729213609761501,
          81.63610411417982,
          0.7369110818101599,
          1.5576113830419072,
          1.7051881244597964,
          2.94607641103301,
          137.0596073139344,
          0.2108849393450216,
          0.5113546594257614,
          1.9978681687804358,
          0.0241347615795246,
          0.4702855172965818,
          0.15566904593471861,
          0.22687189272293876,
          1.1951465218647734,
          4.396055029111435,
          0.1531814099446745,
          3.843821898307243,
          0.4159595698083091,
          1.4319473367282285,
          0.16427029468271315,
          0.9512439641030263,
          0.23337678969746045,
          4.9742812497444415,
          1.6278636081144267,
          3.9656481321373884,
          0.002962206952156506,
          0.8247662473885854,
          0.018356057489468596,
          4.96954283629746,
          0.2526197650891116,
          1.0016191458270625,
          2.159815618744482,
          0.016770992924140082,
          0.16391636763247136,
          1.1758219558248686,
          2.1373127679018933,
          0.0052726151545445235,
          0.08082152188701433,
          2.8546762800384817,
          0.07669364362460926,
          165.28691514155094,
          0.7566003088685966,
          0.3675883933601947,
          0.7561312481971879,
          2.802712909671228,
          0.6552168238294981,
          45.28431369689372,
          2.9442641379652845,
          142.5940769544105,
          0.1900962061675508,
          0.006689273838535277,
          0.8735231095703287,
          0.03578149641774164,
          0.06572002666539688,
          2.09713844740222,
          1.6477356690744671,
          0.5359967392685735,
          0.8644264159293986,
          3.568827948421502,
          0.2741702554352269,
          0.09831721391176691,
          0.40310734195543474,
          0.5552776030180353,
          1.0484580450767127,
          0.07519280578759159,
          1.7777211760037073,
          2.997416598619421,
          310.8116835045075,
          0.6255594877794333,
          0.22903994869506286,
          0.07430897070412856,
          0.20881267801238623,
          0.2524004401404348,
          2.2115167955423303,
          0.2389691690593594,
          1.1356753124906644,
          0.9529784812707214,
          0.7022862425062801,
          3.837668414032274,
          0.2295387729355668,
          0.11686704966854342,
          0.0578437227275557,
          0.8367688718676124,
          0.06029383933404287,
          0.041039580005003275,
          0.37734088713758435,
          0.6965208230284107,
          0.7579684784874885,
          18.531408359860134,
          0.0933398573250355,
          0.35059670108593854,
          2.0567973125194707,
          0.06475938655445641,
          0.9085083136130665,
          1.2307974727380444,
          0.20542240089451919,
          1.8889795638586238,
          0.0023991988060800135,
          0.18459491547647486,
          0.573467688366867,
          0.7890459775630706,
          3.9634531171530782,
          1.846580858061141,
          1.790473659920082,
          0.5052938613527906,
          0.47639439766510355,
          1.9291871211980336,
          0.3811249330626276,
          4.9626914785686065,
          0.23463376930108773,
          0.9549074464011867,
          1.455156675719445,
          0.034740059322889746,
          1.149181977847208,
          1.782460669041946,
          3.1933611301964215,
          0.24404801311074473,
          0.3635350409754257,
          1.2881592061660894,
          1.33736727232945,
          7.262855491105235,
          0.2605460772358663,
          1.605387152894936,
          1.2880647870459052,
          1.9845877539933057,
          0.05691794826082616,
          2.5999855761413615,
          1.9638747058876198,
          26.606571717608464,
          5.065500063559475,
          0.839989261259716,
          24.069801534096992,
          1.0826122774848663,
          1.7506623424545003,
          0.7588193342990067,
          0.3582753737175111,
          4.37674625990225,
          0.13728714028300093,
          2.4844514581569586,
          58.66375951981881,
          1.6235773979265142,
          2.73112913356285,
          0.9006152839831103,
          0.6711662424009789,
          4.010502443431542,
          0.35808732308400554,
          0.3022661462024182,
          28.431087368158252,
          1.147972316308343,
          6.042929802546528,
          2.36477576335162,
          0.0935706501024773,
          1.0712778853172145,
          0.44150442475299645,
          50.65148850167102,
          12.734594994032433,
          2.617054047297445,
          187.96179863169095,
          1.3654955774373785,
          1.1189508377777433,
          3.8616986985741715,
          2.0665368105734117,
          0.23833449865882733,
          1.219444210362066,
          1.764067388443953,
          10.3645185881279,
          2.828882613211992,
          0.5079702517427982,
          1.8778571020490795,
          37.60942183651314,
          0.5931388629795062,
          0.26384617170101377,
          0.6556203346980974,
          4.611177536883005,
          0.9057197806738259,
          0.5664615367319576,
          1.9769737055729573,
          3.7105143241455627,
          0.5642441618258992,
          33.40361905235559,
          0.728636345989412,
          1.0343293612171134,
          1.6829221379094674,
          2.93815254259527,
          1.2710762306739323,
          1.2489617200866507,
          2.4270613600129725,
          6.68196925966331,
          0.3155722062868775,
          0.9333711787859181,
          2.6941845178931,
          2.1704184461859084,
          0.3156916273773215,
          13.45452234779103,
          0.897133112627812,
          0.47669279354226446,
          0.7124695278988952,
          1.7021601679764582,
          0.7674497998817359,
          0.6793139450625187,
          15.667122184542649,
          0.11726485012169882,
          3.540833930139097,
          1.0766097183548873,
          2.1599553705843255,
          0.8101458111404016,
          5.910125562610572,
          49.644523960690265,
          0.20305296311354698,
          1.8056728927339152,
          0.9306230156518418,
          0.6179763760622787,
          0.7461196095762629,
          2.0177743384824205,
          1.8352604437608697,
          0.6304439712282975,
          4.143655471021461,
          20.35974973447071,
          0.01568983847056504,
          106.49484818220228,
          0.9351048263296843,
          4.8133983703429655,
          82.8357612743662,
          0.6625278142087594,
          16.53589223534274,
          0.2749364154145935,
          2.527865558928207,
          69.57292917932025,
          387.6341231868266,
          2.8948830512614645,
          3.113437069515939,
          0.6714565243738069,
          50.74110520988926,
          0.04838211211269483,
          0.5210417343224559,
          49.186344378555575,
          0.5210676593791267,
          14.700681734861819,
          8.72734617708239,
          2.78055796710102,
          0.008713826059292527,
          2.4405750072631354,
          0.6655591028893701,
          231.3733392363793,
          2.3162041854469195,
          117.69355051082435,
          0.09802964267460443,
          2.37831608075244,
          99.11737599436444,
          0.1651558378910869,
          4.183474206201638,
          1.3303011143745085,
          1.182244273970015,
          25.787290050752745,
          1.1903425248511799,
          3.397603907526103,
          0.17930591294904702,
          1.563019806538274,
          0.6227524243824446,
          3.8153835386134904,
          0.18913176521373404,
          3.023289471163567,
          0.06501667626482059,
          0.22983650112380444,
          1.877995347563611,
          1.458041023231678,
          0.6837327370217341,
          0.09392880098054576,
          2.5135577503928426,
          0.37272447651910384,
          9.298640812758325,
          1.878367611415209,
          0.48986421168027383,
          5.610666313428718,
          0.15089159362414648,
          1.1852716996602453,
          22.666532779470685,
          1.5863614101095025,
          2.5396855075550966,
          0.2109655540760388,
          3.661725252144009,
          0.659436710465843,
          1.9249214994968153,
          0.4459935134988219,
          0.25039781470099054,
          0.1747221778905211,
          1.2530657509082848,
          3.4718137812341388,
          0.96457522242252,
          3.7563355003659202,
          2.3531348021345035,
          4.970912155333873,
          0.11668139806913218,
          1.2627988186263308,
          7.485175848816744,
          0.17975724730359344,
          1.963294380362542,
          0.4377813745224608,
          2.0895340968853713,
          282.3202833979377,
          1.464803176008173,
          0.3505928478677483,
          1.257226218407595,
          1.1765454510883855,
          1.374212178197429,
          0.9563789014355627,
          2.9969948396553248,
          6.00785009046201,
          0.7655729074092644,
          0.7448907486325367,
          3.1971561533898667,
          2.7920358517145374,
          0.68245759649199,
          0.3826580828111333,
          1.3369686343309808,
          1.1412074979730713,
          3.0351648695034736,
          0.030462302306058307,
          16.703444657371026,
          74.36983691503823,
          3.473174273639742,
          5.344487920253572,
          0.015756434991377185,
          12.07596619397229,
          1.4344892667693938,
          1.5294399518409958,
          3.853702801016354,
          0.8076613113221113,
          0.4964962100908642,
          4.9462304767673615,
          1.6305692385773003,
          1.3952125091271952,
          1.6302133627227318,
          1.0838554760820105,
          14.488296707696703,
          0.12982618810229904,
          40.01605941995386,
          1.225806132401094,
          0.5468925632952607,
          2.522829421931119,
          1.683206510283501,
          0.4455962777433949,
          31.72770127751508,
          0.292650343970962,
          2.267083744526819,
          0.8460973322333882,
          63.51196319606767,
          2.4746993330430485
         ]
        },
        {
         "mode": "lines",
         "name": "Best Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          7.935048465077573,
          7.935048465077573,
          7.935048465077573,
          7.935048465077573,
          7.935048465077573,
          7.935048465077573,
          7.935048465077573,
          7.935048465077573,
          7.935048465077573,
          7.935048465077573,
          7.935048465077573,
          7.935048465077573,
          5.598382393610107,
          5.598382393610107,
          5.598382393610107,
          5.598382393610107,
          4.849724113540047,
          4.849724113540047,
          4.849724113540047,
          4.849724113540047,
          4.849724113540047,
          4.849724113540047,
          3.900574027615415,
          3.900574027615415,
          3.900574027615415,
          3.900574027615415,
          3.900574027615415,
          3.900574027615415,
          3.900574027615415,
          3.900574027615415,
          3.900574027615415,
          2.6035303780248436,
          2.6035303780248436,
          2.6035303780248436,
          2.2976683955039836,
          2.2976683955039836,
          2.2976683955039836,
          2.2976683955039836,
          2.2976683955039836,
          2.2976683955039836,
          2.2976683955039836,
          2.2976683955039836,
          2.2976683955039836,
          2.2976683955039836,
          2.2976683955039836,
          1.3208993944055742,
          1.3208993944055742,
          1.3208993944055742,
          1.3208993944055742,
          1.3208993944055742,
          1.3208993944055742,
          1.3208993944055742,
          1.3208993944055742,
          1.3208993944055742,
          1.3208993944055742,
          1.3208993944055742,
          0.13144875827851313,
          0.13144875827851313,
          0.13144875827851313,
          0.13144875827851313,
          0.13144875827851313,
          0.13144875827851313,
          0.13144875827851313,
          0.13144875827851313,
          0.13144875827851313,
          0.13144875827851313,
          0.04331950282309835,
          0.04331950282309835,
          0.04331950282309835,
          0.04331950282309835,
          0.04331950282309835,
          0.04331950282309835,
          0.04331950282309835,
          0.017934197711261163,
          0.017934197711261163,
          0.017934197711261163,
          0.017934197711261163,
          0.017934197711261163,
          0.017934197711261163,
          0.017934197711261163,
          0.017934197711261163,
          0.017934197711261163,
          0.017934197711261163,
          0.017934197711261163,
          0.017934197711261163,
          0.017934197711261163,
          0.017934197711261163,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.011397773776140332,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.007708817239161022,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.002962206952156506,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135,
          0.0023991988060800135
         ]
        },
        {
         "marker": {
          "color": "#cccccc"
         },
         "mode": "markers",
         "name": "Infeasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [],
         "y": []
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Optimization History Plot"
        },
        "xaxis": {
         "title": {
          "text": "Trial"
         }
        },
        "yaxis": {
         "title": {
          "text": "Objective Value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 최적화 히스토리 시각화\n",
    "vis.plot_optimization_history(study).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- optuna를 활용한 XGBoost 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-28 14:22:31,507] A new study created in memory with name: no-name-906917ed-2924-4ec2-8eee-ab7fae42112a\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:32,003] Trial 0 finished with value: 0.9624413145539906 and parameters: {'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.021000054051863594, 'colsample_bytree': 0.7944320331334334}. Best is trial 0 with value: 0.9624413145539906.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:32,158] Trial 1 finished with value: 0.9624413145539906 and parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.1341501845475387, 'colsample_bytree': 0.7354020834233754}. Best is trial 0 with value: 0.9624413145539906.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:32,424] Trial 2 finished with value: 0.960093896713615 and parameters: {'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.056413099378549866, 'colsample_bytree': 0.8480915309847677}. Best is trial 0 with value: 0.9624413145539906.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:32,768] Trial 3 finished with value: 0.9577464788732395 and parameters: {'n_estimators': 500, 'max_depth': 10, 'learning_rate': 0.06501469963873868, 'colsample_bytree': 0.7203641509832749}. Best is trial 0 with value: 0.9624413145539906.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:33,095] Trial 4 finished with value: 0.9647887323943664 and parameters: {'n_estimators': 400, 'max_depth': 10, 'learning_rate': 0.06887154084745709, 'colsample_bytree': 0.8137266214033343}. Best is trial 4 with value: 0.9647887323943664.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:33,286] Trial 5 finished with value: 0.9624413145539905 and parameters: {'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.16634149659141212, 'colsample_bytree': 0.8037699960322092}. Best is trial 4 with value: 0.9647887323943664.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:33,507] Trial 6 finished with value: 0.9624413145539906 and parameters: {'n_estimators': 200, 'max_depth': 4, 'learning_rate': 0.03026636679722313, 'colsample_bytree': 0.8357190332129234}. Best is trial 4 with value: 0.9647887323943664.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:33,714] Trial 7 finished with value: 0.9647887323943661 and parameters: {'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.16056971644631138, 'colsample_bytree': 0.640351238765081}. Best is trial 4 with value: 0.9647887323943664.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:33,932] Trial 8 finished with value: 0.9671361502347419 and parameters: {'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.0618615790939648, 'colsample_bytree': 0.6973458582464764}. Best is trial 8 with value: 0.9671361502347419.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:34,068] Trial 9 finished with value: 0.9624413145539906 and parameters: {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.05301590683713614, 'colsample_bytree': 0.6351615358475167}. Best is trial 8 with value: 0.9671361502347419.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:34,185] Trial 10 finished with value: 0.960093896713615 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0986534864623193, 'colsample_bytree': 0.9681319359966181}. Best is trial 8 with value: 0.9671361502347419.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:34,502] Trial 11 finished with value: 0.9671361502347419 and parameters: {'n_estimators': 400, 'max_depth': 8, 'learning_rate': 0.10120298555131174, 'colsample_bytree': 0.571367216541959}. Best is trial 8 with value: 0.9671361502347419.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:34,734] Trial 12 finished with value: 0.9694835680751174 and parameters: {'n_estimators': 300, 'max_depth': 8, 'learning_rate': 0.10400748703215575, 'colsample_bytree': 0.5128886081483499}. Best is trial 12 with value: 0.9694835680751174.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:34,907] Trial 13 finished with value: 0.9647887323943664 and parameters: {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.11461388204572129, 'colsample_bytree': 0.5145974348734417}. Best is trial 12 with value: 0.9694835680751174.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:35,095] Trial 14 finished with value: 0.9647887323943661 and parameters: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.13358561512778766, 'colsample_bytree': 0.509082231459556}. Best is trial 12 with value: 0.9694835680751174.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:35,280] Trial 15 finished with value: 0.9624413145539906 and parameters: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.1978483675820547, 'colsample_bytree': 0.6322828648084202}. Best is trial 12 with value: 0.9694835680751174.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:35,529] Trial 16 finished with value: 0.9624413145539906 and parameters: {'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.08506252302781538, 'colsample_bytree': 0.9161626970174445}. Best is trial 12 with value: 0.9694835680751174.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:35,689] Trial 17 finished with value: 0.960093896713615 and parameters: {'n_estimators': 100, 'max_depth': 9, 'learning_rate': 0.039844075322455264, 'colsample_bytree': 0.6878882806612039}. Best is trial 12 with value: 0.9694835680751174.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:35,867] Trial 18 finished with value: 0.9624413145539906 and parameters: {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.08050133195572219, 'colsample_bytree': 0.5727665098704542}. Best is trial 12 with value: 0.9694835680751174.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:36,125] Trial 19 finished with value: 0.9671361502347419 and parameters: {'n_estimators': 400, 'max_depth': 9, 'learning_rate': 0.12113253161387143, 'colsample_bytree': 0.5697608729969728}. Best is trial 12 with value: 0.9694835680751174.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:36,439] Trial 20 finished with value: 0.9530516431924881 and parameters: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.01589712494396884, 'colsample_bytree': 0.8927960275788576}. Best is trial 12 with value: 0.9694835680751174.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:36,707] Trial 21 finished with value: 0.9671361502347419 and parameters: {'n_estimators': 400, 'max_depth': 8, 'learning_rate': 0.09674709118696054, 'colsample_bytree': 0.5649407541618725}. Best is trial 12 with value: 0.9694835680751174.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:36,961] Trial 22 finished with value: 0.960093896713615 and parameters: {'n_estimators': 300, 'max_depth': 8, 'learning_rate': 0.08499276043703302, 'colsample_bytree': 0.6725111659083427}. Best is trial 12 with value: 0.9694835680751174.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:37,230] Trial 23 finished with value: 0.9647887323943661 and parameters: {'n_estimators': 400, 'max_depth': 9, 'learning_rate': 0.11102799910863444, 'colsample_bytree': 0.5948998714827671}. Best is trial 12 with value: 0.9694835680751174.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:37,549] Trial 24 finished with value: 0.9694835680751174 and parameters: {'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.14342440540427775, 'colsample_bytree': 0.5360195262529929}. Best is trial 12 with value: 0.9694835680751174.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:37,840] Trial 25 finished with value: 0.971830985915493 and parameters: {'n_estimators': 500, 'max_depth': 9, 'learning_rate': 0.1659738245986902, 'colsample_bytree': 0.5046056593621138}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:38,137] Trial 26 finished with value: 0.9624413145539906 and parameters: {'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.1580806817610387, 'colsample_bytree': 0.5276365373089764}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:38,405] Trial 27 finished with value: 0.971830985915493 and parameters: {'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.1789411359614832, 'colsample_bytree': 0.5407029458715945}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:38,682] Trial 28 finished with value: 0.971830985915493 and parameters: {'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.19652109825646757, 'colsample_bytree': 0.5038346152694774}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:38,955] Trial 29 finished with value: 0.9624413145539906 and parameters: {'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.1995221705656047, 'colsample_bytree': 0.6063781005803139}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:39,230] Trial 30 finished with value: 0.9671361502347419 and parameters: {'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.1816929067602847, 'colsample_bytree': 0.7698553377398649}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:39,516] Trial 31 finished with value: 0.971830985915493 and parameters: {'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.18308194572202888, 'colsample_bytree': 0.5366057251735862}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:39,809] Trial 32 finished with value: 0.9671361502347419 and parameters: {'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.180217730893593, 'colsample_bytree': 0.5475642736207303}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:40,091] Trial 33 finished with value: 0.971830985915493 and parameters: {'n_estimators': 500, 'max_depth': 4, 'learning_rate': 0.1823802534914205, 'colsample_bytree': 0.5008322674574943}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:40,373] Trial 34 finished with value: 0.9624413145539906 and parameters: {'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.17305302606703177, 'colsample_bytree': 0.5969100344858307}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:40,599] Trial 35 finished with value: 0.9671361502347419 and parameters: {'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.19004162843499078, 'colsample_bytree': 0.5444028592300172}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:40,879] Trial 36 finished with value: 0.9694835680751174 and parameters: {'n_estimators': 500, 'max_depth': 4, 'learning_rate': 0.14804740498506574, 'colsample_bytree': 0.5413271471904857}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:41,169] Trial 37 finished with value: 0.9671361502347419 and parameters: {'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.17277003644795777, 'colsample_bytree': 0.6059705542715419}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:41,410] Trial 38 finished with value: 0.960093896713615 and parameters: {'n_estimators': 400, 'max_depth': 5, 'learning_rate': 0.15063230963522908, 'colsample_bytree': 0.6569082764789349}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:41,695] Trial 39 finished with value: 0.9671361502347419 and parameters: {'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.18878132334769535, 'colsample_bytree': 0.7491220685840997}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:41,984] Trial 40 finished with value: 0.9694835680751174 and parameters: {'n_estimators': 500, 'max_depth': 4, 'learning_rate': 0.16541366687492756, 'colsample_bytree': 0.5587724077647991}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:42,279] Trial 41 finished with value: 0.9694835680751174 and parameters: {'n_estimators': 500, 'max_depth': 4, 'learning_rate': 0.18474212545754598, 'colsample_bytree': 0.5074929275059007}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:42,557] Trial 42 finished with value: 0.971830985915493 and parameters: {'n_estimators': 500, 'max_depth': 4, 'learning_rate': 0.17559638015602735, 'colsample_bytree': 0.5011309043952058}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:42,777] Trial 43 finished with value: 0.9694835680751174 and parameters: {'n_estimators': 400, 'max_depth': 5, 'learning_rate': 0.19161765148193002, 'colsample_bytree': 0.5336146409477747}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:43,074] Trial 44 finished with value: 0.9694835680751174 and parameters: {'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.15637481385472224, 'colsample_bytree': 0.5024133990091287}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:43,292] Trial 45 finished with value: 0.9624413145539905 and parameters: {'n_estimators': 400, 'max_depth': 3, 'learning_rate': 0.16681260766669997, 'colsample_bytree': 0.5872161164606378}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:43,586] Trial 46 finished with value: 0.9694835680751174 and parameters: {'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.13790668773647805, 'colsample_bytree': 0.522919182302914}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:43,872] Trial 47 finished with value: 0.9647887323943661 and parameters: {'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.1970273155111422, 'colsample_bytree': 0.6247166672273234}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:44,134] Trial 48 finished with value: 0.9647887323943661 and parameters: {'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.16838887529127436, 'colsample_bytree': 0.5512216494604929}. Best is trial 25 with value: 0.971830985915493.\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_1908\\1028312989.py:4: FutureWarning:\n",
      "\n",
      "suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "\n",
      "[I 2025-03-28 14:22:44,400] Trial 49 finished with value: 0.9647887323943661 and parameters: {'n_estimators': 500, 'max_depth': 4, 'learning_rate': 0.1815283093001723, 'colsample_bytree': 0.5266924899494935}. Best is trial 25 with value: 0.971830985915493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 500, 'max_depth': 9, 'learning_rate': 0.1659738245986902, 'colsample_bytree': 0.5046056593621138}\n",
      "0.971830985915493\n"
     ]
    }
   ],
   "source": [
    "# 1. 목적 함수\n",
    "# Optuna는 여러 개의 조합을 테스트하면서 최적의 파라미터를 찾는데, 이때 각 조합에서 성능을 평가할 \"목적 함수\" 가 필요합니다.\n",
    "# trial은 하나의 시도 (하이퍼파라미터 조합)를 의미합니다.\n",
    "def xgb_optuna_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500, 100), # n_estimators: 트리의 개수 (100부터 500까지, 100 단위)\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10), # max_depth: 트리의 최대 깊이 (3~10 사이 정수)\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2), # learning_rate: 학습률 (0.01~0.2 사이 실수)\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0) # colsample_bytree: 트리를 만들 때 사용하는 피처 비율 (0.5~1.0 사이 실수)\n",
    "    }\n",
    "    xgb_clf = XGBClassifier(**params) # 위에서 정해진 하이퍼파라미터를 기반으로 XGBoost 분류기를 만듭니다.\n",
    "    return cross_val_score(xgb_clf, X_train, y_train, scoring='accuracy', cv=3).mean() # 3-겹 교차 검증(cross-validation)을 수행해서 평균 정확도를 구한 뒤 반환합니다.\n",
    "# 이 평균 정확도를 기준으로 Optuna가 \"좋은 파라미터인지\" 판단합니다.\n",
    "\n",
    "# 2. study 객체 -> 최적화\n",
    "study = optuna.create_study(direction='maximize') # 최적화 방향을 \"최대한 높은 정확도\"로 설정합니다 (maximize).\n",
    "study.optimize(xgb_optuna_objective, n_trials=50) # xgb_optuna_objective 목적 함수를 기준으로 50번 시도하면서 최적의 하이퍼파라미터 조합을 찾습니다.\n",
    "\n",
    "\n",
    "# 3. 결과 출력\n",
    "print(study.best_params) # 가장 성능이 좋았던 하이퍼파라미터 조합을 출력합니다.\n",
    "print(study.best_value) # 그 조합으로 얻은 최고 정확도를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다시 한번 더 용어 개념 정리\n",
    "### 파라미터 (Parameter) 란?\n",
    "- 모델이 학습을 통해 스스로 찾아내는 값\n",
    "- 예) 선형회귀: y = wx + b , w(가중치) b(편향) 이 둘이 파라미터\n",
    "- 예) 신경망(딥러닝): 각 층의 가중치(weight) 와 편향(bias), 학습을 통해 업데이트 됨\n",
    "\n",
    "- XGBoost\n",
    "- 여러 개의 결정 트리를 만들 때, 각 트리의 분기 기준, 리프 노드의 값 등 → 학습 데이터를 보고 결정됨\n",
    "\n",
    "- 즉, 모델이 학습하면서 자동으로 조정하는 값들이 파라미터입니다.\n",
    "\n",
    "### 하이퍼파라미터 (Hyperparameter)란?\n",
    "- 사람이 직접 설정해줘야 하는 값\n",
    "- 예시:\n",
    "- 학습률 (learning_rate)\n",
    "\n",
    "- 트리 개수 (n_estimators)\n",
    "\n",
    "- 트리 깊이 (max_depth)\n",
    "\n",
    "- 배치 크기 (batch_size)\n",
    "\n",
    "- 드롭아웃 비율 (dropout)\n",
    "\n",
    "- 정규화 계수 (lambda, alpha)\n",
    "\n",
    "- 활성화 함수 (relu, tanh 등)\n",
    "\n",
    "- 🧑‍💻 즉, 모델이 학습하기 전에 우리가 직접 설정해야 하는 값들이 하이퍼파라미터입니다.\n",
    "\n",
    "- 🎯 왜 하이퍼파라미터 튜닝이 중요할까?\n",
    "- 잘못된 하이퍼파라미터 설정 → 과적합/과소적합, 학습 불안정, 성능 저하\n",
    "\n",
    "- 최적의 하이퍼파라미터 설정 → 일반화 성능 극대화\n",
    "\n",
    "- 그래서 우리가 Optuna, GridSearchCV, RandomSearch 같은 도구로 하이퍼파라미터를 \"자동 탐색\"하려는 거예요.\n",
    "- 🎓 결론\n",
    "- 파라미터는 \"모델이 배우는 값\"\n",
    "\n",
    "- 하이퍼파라미터는 \"우리가 알려주는 설정값\"\n",
    "\n",
    "- 하이퍼파라미터를 잘 조정해야 모델이 좋은 성능을 낼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HyperOpt vs Optuna\n",
    "\n",
    "- HyperOpt\n",
    "    - 'colsample_bytree': np.float64(0.6931939616646488)\n",
    "    - 'learning_rate': np.float64(0.1541783542739828)\n",
    "    - 'max_depth': np.float64(8.0)\n",
    "    - 'n_estimators': np.float64(400.0)\n",
    "\n",
    "- Optuna\n",
    "    - 'n_estimators': 500\n",
    "    - 'max_depth': 9\n",
    "    - 'learning_rate': 0.1659738245986902\n",
    "    - 'colsample_bytree': 0.5046056593621138"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyperOpt 최적 파라미터 적용: 0.958041958041958\n",
      "Optuna 최적 파라미터 적용: 0.958041958041958\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "xgb_hpopt = XGBClassifier(\n",
    "    n_estimators=400, # 트리 400, 깊이 8, 학습률 0.15, 피처 70% 사용\n",
    "    max_depth=8,\n",
    "    learning_rate=0.15,\n",
    "    colsample_bytree=0.7\n",
    ")\n",
    "\n",
    "xgb_optuna = XGBClassifier(  # 트리 500, 깊이 9, 학습률 0.17, 피처 50% 사용\n",
    "    n_estimators=500,\n",
    "    max_depth=9,\n",
    "    learning_rate=0.17,\n",
    "    colsample_bytree=0.5\n",
    ")\n",
    "\n",
    "xgb_hpopt.fit(X_train, y_train) # 두 모델 모두 학습 데이터 (X_train, y_train)로 학습을 진행함\n",
    "xgb_optuna.fit(X_train, y_train)\n",
    "\n",
    "hpopt_pred = xgb_hpopt.predict(X_test) # 학습된 모델들이 테스트 데이터(X_test)에 대해 예측값을 만듦\n",
    "optuna_pred = xgb_optuna.predict(X_test)\n",
    "\n",
    "print(f\"HyperOpt 최적 파라미터 적용: {accuracy_score(y_test, hpopt_pred)}\")\n",
    "print(f\"Optuna 최적 파라미터 적용: {accuracy_score(y_test, optuna_pred)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
